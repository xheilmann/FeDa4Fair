{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_income(df):\n",
    "    \"\"\"\n",
    "    Pre-process the income dataset to make it ready for the simulation\n",
    "    In this function we consider \"SEX\" as the sensitive value and \"PINCP\" as the target value.\n",
    "\n",
    "    Args:\n",
    "        data: the raw data\n",
    "        years_list: the list of years to be considered\n",
    "        states_list: the list of states to be considered\n",
    "\n",
    "    Returns:\n",
    "        Returns a list of pre-processed data for each state, if multiple years are\n",
    "        selected, the data are concatenated.\n",
    "        We return three lists:\n",
    "        - The first list contains a pandas dataframe of features for each state\n",
    "        - The second list contains a pandas dataframe of labels for each state\n",
    "        - The third list contains a pandas dataframe of groups for each state\n",
    "        The values in the list are numpy array of the dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    categorical_columns = [\"COW\", \"SCHL\"] #, \"RAC1P\"]\n",
    "    continuous_columns = [\"AGEP\", \"WKHP\", \"OCCP\", \"POBP\", \"RELP\"]\n",
    "\n",
    "    # get the target and sensitive attributes\n",
    "    target_attributes = df[\">50K\"]\n",
    "    sensitive_attributes = df[\"SEX\"]\n",
    "\n",
    "    # convert the columns to one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dtype=int)\n",
    "\n",
    "    # normalize the continuous columns between 0 and 1\n",
    "    for col in continuous_columns:\n",
    "        df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "\n",
    "def pre_process_single_datasets(df):\n",
    "    dataframe = pd.DataFrame()\n",
    "    label = pd.DataFrame()\n",
    "    group = pd.DataFrame()\n",
    "    second_group = pd.DataFrame()\n",
    "    third_group = pd.DataFrame()\n",
    "    dataframes = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    second_groups = []\n",
    "    third_groups = []\n",
    "    target_attributes = df[\">50K\"]\n",
    "    sensitive_attributes = df[\"SEX\"]\n",
    "    second_sensitive_attributes = df[\"MAR\"]\n",
    "    \n",
    "    third_sensitive_attributes = df[\"RAC1P\"]\n",
    "    third_sensitive_attributes = third_sensitive_attributes.astype(int)\n",
    "    target_attributes = target_attributes.astype(int)\n",
    "\n",
    "    sensitive_attributes = [1 if item == 1 else 0 for item in sensitive_attributes]\n",
    "\n",
    "    second_sensitive_attributes = [\n",
    "        1 if item == 1 else 0 for item in second_sensitive_attributes\n",
    "    ]\n",
    "\n",
    "    third_sensitive_attributes = [\n",
    "        1 if item == 1 else 0 for item in third_sensitive_attributes\n",
    "    ]\n",
    "\n",
    "    df = df.drop([\">50K\"], axis=1)\n",
    "    # df.drop(['RAC1P_1.0', 'RAC1P_2.0'], axis=1, inplace=True)\n",
    "\n",
    "    # concatenate the dataframes\n",
    "    dataframe = pd.concat([dataframe, df])\n",
    "    # remove RAC1P from dataframe\n",
    "\n",
    "    # convert the labels and groups to dataframes\n",
    "    label = pd.concat([label, pd.DataFrame(target_attributes)])\n",
    "    group = pd.concat([group, pd.DataFrame(sensitive_attributes)])\n",
    "    second_group = pd.concat([second_group, pd.DataFrame(second_sensitive_attributes)])\n",
    "    third_group = pd.concat([third_group, pd.DataFrame(third_sensitive_attributes)])\n",
    "\n",
    "    assert len(dataframe) == len(label) == len(group) == len(second_group)\n",
    "    dataframes.append(dataframe.to_numpy())\n",
    "    labels.append(label.to_numpy())\n",
    "    groups.append(group.to_numpy())\n",
    "    second_groups.append(second_group.to_numpy())\n",
    "    third_groups.append(third_group.to_numpy())\n",
    "    return dataframes, labels, groups, second_groups, third_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AK_0.csv', 'AL_0.csv', 'AR_0.csv', 'AZ_0.csv', 'CA_0.csv', 'CO_0.csv', 'CT_0.csv', 'DE_0.csv', 'FL_0.csv', 'FL_data', 'GA_0.csv', 'HI_0.csv', 'IA_0.csv', 'ID_0.csv', 'IL_0.csv', 'IN_0.csv', 'KS_0.csv', 'KY_0.csv', 'LA_0.csv', 'MA_0.csv', 'MD_0.csv', 'ME_0.csv', 'MI_0.csv', 'MN_0.csv', 'MO_0.csv', 'MS_0.csv', 'MT_0.csv', 'NC_0.csv', 'ND_0.csv', 'NE_0.csv', 'NH_0.csv', 'NJ_0.csv', 'NM_0.csv', 'NV_0.csv', 'NY_0.csv', 'OH_0.csv', 'OK_0.csv', 'OR_0.csv', 'PA_0.csv', 'PR_0.csv', 'RI_0.csv', 'SC_0.csv', 'SD_0.csv', 'TN_0.csv', 'TX_0.csv', 'UT_0.csv', 'VA_0.csv', 'VT_0.csv', 'WA_0.csv', 'WI_0.csv', 'WV_0.csv', 'WY_0.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CT_0',\n",
       " 'RI_0',\n",
       " 'VT_0',\n",
       " 'TX_0',\n",
       " 'GA_0',\n",
       " 'PR_0',\n",
       " 'OH_0',\n",
       " 'NE_0',\n",
       " 'HI_0',\n",
       " 'MO_0',\n",
       " 'PA_0',\n",
       " 'DE_0',\n",
       " 'WV_0',\n",
       " 'MD_0',\n",
       " 'AZ_0',\n",
       " 'LA_0',\n",
       " 'WA_0',\n",
       " 'TN_0',\n",
       " 'MA_0',\n",
       " 'NJ_0',\n",
       " 'ME_0',\n",
       " 'SC_0',\n",
       " 'MI_0',\n",
       " 'OK_0',\n",
       " 'IL_0',\n",
       " 'FL_0',\n",
       " 'UT_0',\n",
       " 'AK_0',\n",
       " 'WI_0',\n",
       " 'NH_0',\n",
       " 'VA_0',\n",
       " 'SD_0',\n",
       " 'MS_0',\n",
       " 'ND_0',\n",
       " 'NC_0',\n",
       " 'AL_0',\n",
       " 'IA_0',\n",
       " 'ID_0',\n",
       " 'WY_0',\n",
       " 'NV_0',\n",
       " 'NM_0',\n",
       " 'NY_0',\n",
       " 'CA_0',\n",
       " 'AR_0',\n",
       " 'MN_0',\n",
       " 'OR_0',\n",
       " 'MT_0',\n",
       " 'KY_0',\n",
       " 'KS_0',\n",
       " 'IN_0',\n",
       " 'CO_0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "folder = \"../data/cross_silo_attribute_final/\"\n",
    "# folder = \"../data/cross_silo_value_final/\"\n",
    "list_files = !ls {folder}\n",
    "unfair_dfs = []\n",
    "print(list_files)\n",
    "\n",
    "states = ['CT',\n",
    " 'RI',\n",
    " 'VT',\n",
    " 'TX',\n",
    " 'GA',\n",
    " 'PR',\n",
    " 'OH',\n",
    " 'NE',\n",
    " 'HI',\n",
    " 'MO',\n",
    " 'PA',\n",
    " 'DE',\n",
    " 'WV',\n",
    " 'MD',\n",
    " 'AZ',\n",
    " 'LA',\n",
    " 'WA',\n",
    " 'TN',\n",
    " 'MA',\n",
    " 'NJ',\n",
    " 'ME',\n",
    " 'SC',\n",
    " 'MI',\n",
    " 'OK',\n",
    " 'IL',\n",
    " 'FL',\n",
    " 'UT',\n",
    " 'AK',\n",
    " 'WI',\n",
    " 'NH',\n",
    " 'VA',\n",
    " 'SD',\n",
    " 'MS',\n",
    " 'ND',\n",
    " 'NC',\n",
    " 'AL',\n",
    " 'IA',\n",
    " 'ID',\n",
    " 'WY',\n",
    " 'NV',\n",
    " 'NM',\n",
    " 'NY',\n",
    " 'CA',\n",
    " 'AR',\n",
    " 'MN',\n",
    " 'OR',\n",
    " 'MT',\n",
    " 'KY',\n",
    " 'KS',\n",
    " 'IN',\n",
    " 'CO']\n",
    "\n",
    "partitions_names = []\n",
    "\n",
    "for state in states:\n",
    "    partitions = set()\n",
    "    for file in list_files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            partition = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "            if partition not in partitions:\n",
    "                partitions_names.append(f\"{state}_{partition}\")\n",
    "                partitions.add(partition)\n",
    "                train = pd.read_csv(f\"{folder}{state}_{partition}.csv\")\n",
    "                # split the train csv into train and test\n",
    "                train, test = train_test_split(train, test_size=0.2)\n",
    "\n",
    "                # val = pd.read_csv(f\"../data/{state}_val_{partition}.csv\")\n",
    "                # concatenated_data = pd.concat([train, val])\n",
    "                unfair_dfs.append(train)\n",
    "                # df = pd.read_csv(f\"../data/{state}_test_{partition}.csv\")\n",
    "                unfair_dfs.append(test)\n",
    "    \n",
    "partitions_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(unfair_dfs, ignore_index=True)\n",
    "concatenated_df[\"PINCP\"] = [1 if item == True else 0 for item in concatenated_df[\"PINCP\"]]\n",
    "\n",
    "# rename the column PINCP to >50K\n",
    "concatenated_df.rename(columns={\"PINCP\": \">50K\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.drop([\"__index_level_0__\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>POBP</th>\n",
       "      <th>RELP</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>&gt;50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4110.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626653</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5940.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626654</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9130.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626655</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626656</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626657</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4220.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1626658 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AGEP  COW  SCHL  MAR    OCCP   POBP  RELP  WKHP  SEX  RAC1P  >50K\n",
       "0        22.0  1.0  16.0  2.0  4055.0    9.0  16.0  12.0  1.0    2.0     0\n",
       "1        17.0  1.0  14.0  2.0  4110.0    6.0   2.0   6.0  1.0    1.0     0\n",
       "2        51.0  1.0  19.0  1.0  3500.0  303.0   1.0  40.0  2.0    1.0     0\n",
       "3        31.0  1.0  24.0  1.0  1650.0   25.0   0.0  50.0  2.0    1.0     0\n",
       "4        63.0  7.0  16.0  1.0   440.0    9.0   0.0  70.0  1.0    1.0     1\n",
       "...       ...  ...   ...  ...     ...    ...   ...   ...  ...    ...   ...\n",
       "1626653  59.0  1.0  20.0  2.0  5940.0  110.0   0.0  40.0  2.0    1.0     0\n",
       "1626654  61.0  1.0  19.0  2.0  9130.0    8.0   0.0  40.0  1.0    1.0     0\n",
       "1626655  44.0  1.0  21.0  1.0   440.0   18.0   1.0  45.0  1.0    1.0     0\n",
       "1626656  39.0  1.0  21.0  1.0  4700.0   10.0   0.0  30.0  2.0    1.0     0\n",
       "1626657  19.0  1.0  17.0  2.0  4220.0   36.0  12.0  40.0  1.0    1.0     0\n",
       "\n",
       "[1626658 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15828"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfair_dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfair_dfs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "pre_processed_df = pre_process_income(concatenated_df)\n",
    "\n",
    "split_dfs = []\n",
    "start_idx = 0\n",
    "for df in unfair_dfs:\n",
    "    end_idx = start_idx + len(df)\n",
    "    split_dfs.append(pre_processed_df.iloc[start_idx:end_idx])\n",
    "    start_idx = end_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15828 3957\n",
      "0 (15828, 40) (3957, 40)\n",
      "4569 1143\n",
      "1 (4569, 40) (1143, 40)\n",
      "3008 753\n",
      "2 (3008, 40) (753, 40)\n",
      "102981 25746\n",
      "3 (102981, 40) (25746, 40)\n",
      "40378 10095\n",
      "4 (40378, 40) (10095, 40)\n",
      "7092 1774\n",
      "5 (7092, 40) (1774, 40)\n",
      "46666 11667\n",
      "6 (46666, 40) (11667, 40)\n",
      "8628 2157\n",
      "7 (8628, 40) (2157, 40)\n",
      "6184 1547\n",
      "8 (6184, 40) (1547, 40)\n",
      "23646 5912\n",
      "9 (23646, 40) (5912, 40)\n",
      "51803 12951\n",
      "10 (51803, 40) (12951, 40)\n",
      "3660 916\n",
      "11 (3660, 40) (916, 40)\n",
      "6245 1562\n",
      "12 (6245, 40) (1562, 40)\n",
      "26433 6609\n",
      "13 (26433, 40) (6609, 40)\n",
      "26621 6656\n",
      "14 (26621, 40) (6656, 40)\n",
      "16354 4089\n",
      "15 (16354, 40) (4089, 40)\n",
      "30444 7612\n",
      "16 (30444, 40) (7612, 40)\n",
      "25729 6433\n",
      "17 (25729, 40) (6433, 40)\n",
      "32091 8023\n",
      "18 (32091, 40) (8023, 40)\n",
      "38224 9557\n",
      "19 (38224, 40) (9557, 40)\n",
      "5601 1401\n",
      "20 (5601, 40) (1401, 40)\n",
      "19903 4976\n",
      "21 (19903, 40) (4976, 40)\n",
      "37666 9417\n",
      "22 (37666, 40) (9417, 40)\n",
      "13848 3462\n",
      "23 (13848, 40) (3462, 40)\n",
      "51248 12812\n",
      "24 (51248, 40) (12812, 40)\n",
      "79140 19785\n",
      "25 (79140, 40) (19785, 40)\n",
      "13069 3268\n",
      "26 (13069, 40) (3268, 40)\n",
      "2836 710\n",
      "27 (2836, 40) (710, 40)\n",
      "26112 6529\n",
      "28 (26112, 40) (6529, 40)\n",
      "6099 1525\n",
      "29 (6099, 40) (1525, 40)\n",
      "33768 8442\n",
      "30 (33768, 40) (8442, 40)\n",
      "3919 980\n",
      "31 (3919, 40) (980, 40)\n",
      "10434 2609\n",
      "32 (10434, 40) (2609, 40)\n",
      "3527 882\n",
      "33 (3527, 40) (882, 40)\n",
      "41653 10414\n",
      "34 (41653, 40) (10414, 40)\n",
      "17733 4434\n",
      "35 (17733, 40) (4434, 40)\n",
      "14196 3549\n",
      "36 (14196, 40) (3549, 40)\n",
      "6392 1598\n",
      "37 (6392, 40) (1598, 40)\n",
      "2424 607\n",
      "38 (2424, 40) (607, 40)\n",
      "11845 2962\n",
      "39 (11845, 40) (2962, 40)\n",
      "6968 1743\n",
      "40 (6968, 40) (1743, 40)\n",
      "82416 20605\n",
      "41 (82416, 40) (20605, 40)\n",
      "154397 38600\n",
      "42 (154397, 40) (38600, 40)\n",
      "10732 2684\n",
      "43 (10732, 40) (2684, 40)\n",
      "24816 6205\n",
      "44 (24816, 40) (6205, 40)\n",
      "17369 4343\n",
      "45 (17369, 40) (4343, 40)\n",
      "4370 1093\n",
      "46 (4370, 40) (1093, 40)\n",
      "17571 4393\n",
      "47 (17571, 40) (4393, 40)\n",
      "12141 3036\n",
      "48 (12141, 40) (3036, 40)\n",
      "27481 6871\n",
      "49 (27481, 40) (6871, 40)\n",
      "25044 6262\n",
      "50 (25044, 40) (6262, 40)\n"
     ]
    }
   ],
   "source": [
    "for index in range(0, len(split_dfs), 2):\n",
    "    train_state = split_dfs[index]\n",
    "    test_state = split_dfs[index + 1]\n",
    "    print(len(train_state), len(test_state))\n",
    "    (\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        train_groups,\n",
    "        train_second_groups,\n",
    "        train_third_groups,\n",
    "    ) = pre_process_single_datasets(train_state)\n",
    "    (\n",
    "        test_data,\n",
    "        test_labels,\n",
    "        test_groups,\n",
    "        test_second_groups,\n",
    "        test_third_groups,\n",
    "    ) = pre_process_single_datasets(test_state)\n",
    "\n",
    "    print(index // 2, train_data[0].shape, test_data[0].shape)\n",
    "\n",
    "    if not os.path.exists(\n",
    "        f\"{folder}FL_data/federated/{index // 2}\"\n",
    "    ):\n",
    "        os.makedirs(f\"{folder}FL_data/federated/{index // 2}\")\n",
    "        # save partitions_names \n",
    "    json_file = {index:data for index, data in enumerate(partitions_names)}\n",
    "    with open(f\"{folder}FL_data/federated/partitions_names.json\", \"w\") as f:\n",
    "        json.dump(json_file, f)\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_dataframes_{index // 2}_train.npy\",\n",
    "        train_data[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_labels_{index // 2}_train.npy\",\n",
    "        train_labels[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_groups_{index // 2}_train.npy\",\n",
    "        train_groups[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_second_groups_{index // 2}_train.npy\",\n",
    "        train_second_groups[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_third_groups_{index // 2}_train.npy\",\n",
    "        train_third_groups[0],\n",
    "    )\n",
    "\n",
    "\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_dataframes_{index // 2}_test.npy\",\n",
    "        test_data[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_labels_{index // 2}_test.npy\",\n",
    "        test_labels[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_groups_{index // 2}_test.npy\",\n",
    "        test_groups[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_second_groups_{index // 2}_test.npy\",\n",
    "        test_second_groups[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"{folder}FL_data/federated/{index // 2}/income_third_groups_{index // 2}_test.npy\",\n",
    "        test_third_groups[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25044, 40)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06329114, 2.        , 0.41191446, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.        , 0.41751527, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.43037975, 1.        , 0.35539715, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.17721519, 2.        , 0.4287169 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.5443038 , 2.        , 0.53258656, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.55696203, 1.        , 0.53258656, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_dataframes_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_labels_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_second_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_second_groups_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_third_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_third_groups_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_groups_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_labels_0_test.npy\", allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
