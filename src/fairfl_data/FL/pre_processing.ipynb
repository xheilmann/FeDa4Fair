{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_income(df):\n",
    "    \"\"\"\n",
    "    Pre-process the income dataset to make it ready for the simulation\n",
    "    In this function we consider \"SEX\" as the sensitive value and \"PINCP\" as the target value.\n",
    "\n",
    "    Args:\n",
    "        data: the raw data\n",
    "        years_list: the list of years to be considered\n",
    "        states_list: the list of states to be considered\n",
    "\n",
    "    Returns:\n",
    "        Returns a list of pre-processed data for each state, if multiple years are\n",
    "        selected, the data are concatenated.\n",
    "        We return three lists:\n",
    "        - The first list contains a pandas dataframe of features for each state\n",
    "        - The second list contains a pandas dataframe of labels for each state\n",
    "        - The third list contains a pandas dataframe of groups for each state\n",
    "        The values in the list are numpy array of the dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    categorical_columns = [\"COW\", \"SCHL\"] #, \"RAC1P\"]\n",
    "    continuous_columns = [\"AGEP\", \"WKHP\", \"OCCP\", \"POBP\", \"RELP\"]\n",
    "\n",
    "    # get the target and sensitive attributes\n",
    "    target_attributes = df[\">50K\"]\n",
    "    sensitive_attributes = df[\"SEX\"]\n",
    "\n",
    "    # convert the columns to one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dtype=int)\n",
    "\n",
    "    # normalize the continuous columns between 0 and 1\n",
    "    for col in continuous_columns:\n",
    "        df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "\n",
    "def pre_process_single_datasets(df):\n",
    "    dataframe = pd.DataFrame()\n",
    "    label = pd.DataFrame()\n",
    "    group = pd.DataFrame()\n",
    "    second_group = pd.DataFrame()\n",
    "    third_group = pd.DataFrame()\n",
    "    dataframes = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    second_groups = []\n",
    "    third_groups = []\n",
    "    target_attributes = df[\">50K\"]\n",
    "    sensitive_attributes = df[\"SEX\"]\n",
    "    second_sensitive_attributes = df[\"MAR\"]\n",
    "    \n",
    "    third_sensitive_attributes = df[\"RAC1P\"]\n",
    "    third_sensitive_attributes = third_sensitive_attributes.astype(int)\n",
    "    target_attributes = target_attributes.astype(int)\n",
    "\n",
    "    sensitive_attributes = [1 if item == 1 else 0 for item in sensitive_attributes]\n",
    "\n",
    "    second_sensitive_attributes = [\n",
    "        1 if item == 1 else 0 for item in second_sensitive_attributes\n",
    "    ]\n",
    "\n",
    "    # third_sensitive_attributes = [\n",
    "    #     1 if item == 1 else 0 for item in third_sensitive_attributes\n",
    "    # ]\n",
    "\n",
    "    df = df.drop([\">50K\"], axis=1)\n",
    "    # df.drop(['RAC1P_1.0', 'RAC1P_2.0'], axis=1, inplace=True)\n",
    "\n",
    "    # concatenate the dataframes\n",
    "    dataframe = pd.concat([dataframe, df])\n",
    "    # remove RAC1P from dataframe\n",
    "\n",
    "    # convert the labels and groups to dataframes\n",
    "    label = pd.concat([label, pd.DataFrame(target_attributes)])\n",
    "    group = pd.concat([group, pd.DataFrame(sensitive_attributes)])\n",
    "    second_group = pd.concat([second_group, pd.DataFrame(second_sensitive_attributes)])\n",
    "    third_group = pd.concat([third_group, pd.DataFrame(third_sensitive_attributes)])\n",
    "\n",
    "    assert len(dataframe) == len(label) == len(group) == len(second_group)\n",
    "    dataframes.append(dataframe.to_numpy())\n",
    "    labels.append(label.to_numpy())\n",
    "    groups.append(group.to_numpy())\n",
    "    second_groups.append(second_group.to_numpy())\n",
    "    third_groups.append(third_group.to_numpy())\n",
    "    return dataframes, labels, groups, second_groups, third_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AK_0.csv', 'AL_0.csv', 'AR_0.csv', 'AZ_0.csv', 'CA_0.csv', 'CO_0.csv', 'CT_0.csv', 'DE_0.csv', 'FL_0.csv', 'FL_data', 'GA_0.csv', 'HI_0.csv', 'IA_0.csv', 'ID_0.csv', 'IL_0.csv', 'IN_0.csv', 'KS_0.csv', 'KY_0.csv', 'LA_0.csv', 'MA_0.csv', 'MD_0.csv', 'ME_0.csv', 'MI_0.csv', 'MN_0.csv', 'MO_0.csv', 'MS_0.csv', 'MT_0.csv', 'NC_0.csv', 'ND_0.csv', 'NE_0.csv', 'NH_0.csv', 'NJ_0.csv', 'NM_0.csv', 'NV_0.csv', 'NY_0.csv', 'OH_0.csv', 'OK_0.csv', 'OR_0.csv', 'PA_0.csv', 'PR_0.csv', 'RI_0.csv', 'SC_0.csv', 'SD_0.csv', 'TN_0.csv', 'TX_0.csv', 'UT_0.csv', 'VA_0.csv', 'VT_0.csv', 'WA_0.csv', 'WI_0.csv', 'WV_0.csv', 'WY_0.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CT_0',\n",
       " 'RI_0',\n",
       " 'VT_0',\n",
       " 'TX_0',\n",
       " 'GA_0',\n",
       " 'PR_0',\n",
       " 'OH_0',\n",
       " 'NE_0',\n",
       " 'HI_0',\n",
       " 'MO_0',\n",
       " 'PA_0',\n",
       " 'DE_0',\n",
       " 'WV_0',\n",
       " 'MD_0',\n",
       " 'AZ_0',\n",
       " 'LA_0',\n",
       " 'WA_0',\n",
       " 'TN_0',\n",
       " 'MA_0',\n",
       " 'NJ_0',\n",
       " 'ME_0',\n",
       " 'SC_0',\n",
       " 'MI_0',\n",
       " 'OK_0',\n",
       " 'IL_0',\n",
       " 'FL_0',\n",
       " 'UT_0',\n",
       " 'AK_0',\n",
       " 'WI_0',\n",
       " 'NH_0',\n",
       " 'VA_0',\n",
       " 'SD_0',\n",
       " 'MS_0',\n",
       " 'ND_0',\n",
       " 'NC_0',\n",
       " 'AL_0',\n",
       " 'IA_0',\n",
       " 'ID_0',\n",
       " 'WY_0',\n",
       " 'NV_0',\n",
       " 'NM_0',\n",
       " 'NY_0',\n",
       " 'CA_0',\n",
       " 'AR_0',\n",
       " 'MN_0',\n",
       " 'OR_0',\n",
       " 'MT_0',\n",
       " 'KY_0',\n",
       " 'KS_0',\n",
       " 'IN_0',\n",
       " 'CO_0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "cross_silo = True\n",
    "# folder = \"../data/cross_device_attribute_final/\"\n",
    "# folder = \"../data/cross_device_value_final/\"\n",
    "folder = \"../data/cross_silo_attribute_final/\"\n",
    "# folder = \"../data/cross_silo_value_final/\"\n",
    "list_files = !ls {folder}\n",
    "unfair_dfs = []\n",
    "print(list_files)\n",
    "\n",
    "states = ['CT',\n",
    " 'RI',\n",
    " 'VT',\n",
    " 'TX',\n",
    " 'GA',\n",
    " 'PR',\n",
    " 'OH',\n",
    " 'NE',\n",
    " 'HI',\n",
    " 'MO',\n",
    " 'PA',\n",
    " 'DE',\n",
    " 'WV',\n",
    " 'MD',\n",
    " 'AZ',\n",
    " 'LA',\n",
    " 'WA',\n",
    " 'TN',\n",
    " 'MA',\n",
    " 'NJ',\n",
    " 'ME',\n",
    " 'SC',\n",
    " 'MI',\n",
    " 'OK',\n",
    " 'IL',\n",
    " 'FL',\n",
    " 'UT',\n",
    " 'AK',\n",
    " 'WI',\n",
    " 'NH',\n",
    " 'VA',\n",
    " 'SD',\n",
    " 'MS',\n",
    " 'ND',\n",
    " 'NC',\n",
    " 'AL',\n",
    " 'IA',\n",
    " 'ID',\n",
    " 'WY',\n",
    " 'NV',\n",
    " 'NM',\n",
    " 'NY',\n",
    " 'CA',\n",
    " 'AR',\n",
    " 'MN',\n",
    " 'OR',\n",
    " 'MT',\n",
    " 'KY',\n",
    " 'KS',\n",
    " 'IN',\n",
    " 'CO']\n",
    "\n",
    "partitions_names = []\n",
    "\n",
    "for state in states:\n",
    "    partitions = set()\n",
    "    for file in list_files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            partition = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "            if partition not in partitions:\n",
    "                partitions_names.append(f\"{state}_{partition}\")\n",
    "                partitions.add(partition)\n",
    "                try:\n",
    "                    train = pd.read_csv(f\"{folder}{state}_{partition}.csv\")\n",
    "                    # split the train csv into train and test\n",
    "                    if cross_silo:\n",
    "                        train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "                    # val = pd.read_csv(f\"../data/{state}_val_{partition}.csv\")\n",
    "                    # concatenated_data = pd.concat([train, val])\n",
    "                    unfair_dfs.append(train)\n",
    "                    # df = pd.read_csv(f\"../data/{state}_test_{partition}.csv\")\n",
    "                    if cross_silo:\n",
    "                        unfair_dfs.append(test)\n",
    "                except:\n",
    "                    print(f\"Error reading file {state}_{partition}.csv\")\n",
    "                    continue\n",
    "    \n",
    "partitions_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfair_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(unfair_dfs, ignore_index=True)\n",
    "concatenated_df[\"PINCP\"] = [1 if item == True else 0 for item in concatenated_df[\"PINCP\"]]\n",
    "\n",
    "# rename the column PINCP to >50K\n",
    "concatenated_df.rename(columns={\"PINCP\": \">50K\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df[\"RAC1P\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.drop([\"__index_level_0__\"], axis=1, inplace=True)\n",
    "# concatenated_df.drop([\"__index_level_0__\", \"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>POBP</th>\n",
       "      <th>RELP</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>&gt;50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7315.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5610.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626653</th>\n",
       "      <td>46.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4252.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626654</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626655</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4720.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626656</th>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626657</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1626658 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AGEP  COW  SCHL  MAR    OCCP   POBP  RELP  WKHP  SEX  RAC1P  >50K\n",
       "0        66.0  1.0  16.0  1.0  7315.0    9.0   0.0  35.0  1.0    1.0     1\n",
       "1        52.0  1.0  17.0  2.0  5610.0   53.0   0.0   6.0  2.0    1.0     0\n",
       "2        63.0  1.0  20.0  1.0  3620.0  301.0   1.0  40.0  2.0    1.0     1\n",
       "3        64.0  1.0  16.0  2.0  4030.0    9.0   5.0  32.0  2.0    1.0     0\n",
       "4        26.0  1.0  24.0  2.0  3050.0    9.0   0.0  37.0  2.0    2.0     1\n",
       "...       ...  ...   ...  ...     ...    ...   ...   ...  ...    ...   ...\n",
       "1626653  46.0  3.0  22.0  1.0  4252.0   24.0   0.0  60.0  1.0    1.0     1\n",
       "1626654  20.0  3.0  19.0  2.0  2310.0  303.0  13.0  20.0  1.0    1.0     0\n",
       "1626655  50.0  1.0  20.0  2.0  4720.0    2.0  13.0  40.0  1.0    1.0     1\n",
       "1626656  21.0  4.0  19.0  2.0  2435.0   53.0   0.0  17.0  2.0    1.0     0\n",
       "1626657  23.0  1.0  19.0  2.0  2360.0   17.0   2.0  40.0  1.0    1.0     0\n",
       "\n",
       "[1626658 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15828"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfair_dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unfair_dfs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "pre_processed_df = pre_process_income(concatenated_df)\n",
    "\n",
    "split_dfs = []\n",
    "start_idx = 0\n",
    "for df in unfair_dfs:\n",
    "    end_idx = start_idx + len(df)\n",
    "    split_dfs.append(pre_processed_df.iloc[start_idx:end_idx])\n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(split_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15828 3957\n",
      "0 (15828, 40) (3957, 40)\n",
      "4569 1143\n",
      "1 (4569, 40) (1143, 40)\n",
      "3008 753\n",
      "2 (3008, 40) (753, 40)\n",
      "102981 25746\n",
      "3 (102981, 40) (25746, 40)\n",
      "40378 10095\n",
      "4 (40378, 40) (10095, 40)\n",
      "7092 1774\n",
      "5 (7092, 40) (1774, 40)\n",
      "46666 11667\n",
      "6 (46666, 40) (11667, 40)\n",
      "8628 2157\n",
      "7 (8628, 40) (2157, 40)\n",
      "6184 1547\n",
      "8 (6184, 40) (1547, 40)\n",
      "23646 5912\n",
      "9 (23646, 40) (5912, 40)\n",
      "51803 12951\n",
      "10 (51803, 40) (12951, 40)\n",
      "3660 916\n",
      "11 (3660, 40) (916, 40)\n",
      "6245 1562\n",
      "12 (6245, 40) (1562, 40)\n",
      "26433 6609\n",
      "13 (26433, 40) (6609, 40)\n",
      "26621 6656\n",
      "14 (26621, 40) (6656, 40)\n",
      "16354 4089\n",
      "15 (16354, 40) (4089, 40)\n",
      "30444 7612\n",
      "16 (30444, 40) (7612, 40)\n",
      "25729 6433\n",
      "17 (25729, 40) (6433, 40)\n",
      "32091 8023\n",
      "18 (32091, 40) (8023, 40)\n",
      "38224 9557\n",
      "19 (38224, 40) (9557, 40)\n",
      "5601 1401\n",
      "20 (5601, 40) (1401, 40)\n",
      "19903 4976\n",
      "21 (19903, 40) (4976, 40)\n",
      "37666 9417\n",
      "22 (37666, 40) (9417, 40)\n",
      "13848 3462\n",
      "23 (13848, 40) (3462, 40)\n",
      "51248 12812\n",
      "24 (51248, 40) (12812, 40)\n",
      "79140 19785\n",
      "25 (79140, 40) (19785, 40)\n",
      "13069 3268\n",
      "26 (13069, 40) (3268, 40)\n",
      "2836 710\n",
      "27 (2836, 40) (710, 40)\n",
      "26112 6529\n",
      "28 (26112, 40) (6529, 40)\n",
      "6099 1525\n",
      "29 (6099, 40) (1525, 40)\n",
      "33768 8442\n",
      "30 (33768, 40) (8442, 40)\n",
      "3919 980\n",
      "31 (3919, 40) (980, 40)\n",
      "10434 2609\n",
      "32 (10434, 40) (2609, 40)\n",
      "3527 882\n",
      "33 (3527, 40) (882, 40)\n",
      "41653 10414\n",
      "34 (41653, 40) (10414, 40)\n",
      "17733 4434\n",
      "35 (17733, 40) (4434, 40)\n",
      "14196 3549\n",
      "36 (14196, 40) (3549, 40)\n",
      "6392 1598\n",
      "37 (6392, 40) (1598, 40)\n",
      "2424 607\n",
      "38 (2424, 40) (607, 40)\n",
      "11845 2962\n",
      "39 (11845, 40) (2962, 40)\n",
      "6968 1743\n",
      "40 (6968, 40) (1743, 40)\n",
      "82416 20605\n",
      "41 (82416, 40) (20605, 40)\n",
      "154397 38600\n",
      "42 (154397, 40) (38600, 40)\n",
      "10732 2684\n",
      "43 (10732, 40) (2684, 40)\n",
      "24816 6205\n",
      "44 (24816, 40) (6205, 40)\n",
      "17369 4343\n",
      "45 (17369, 40) (4343, 40)\n",
      "4370 1093\n",
      "46 (4370, 40) (1093, 40)\n",
      "17571 4393\n",
      "47 (17571, 40) (4393, 40)\n",
      "12141 3036\n",
      "48 (12141, 40) (3036, 40)\n",
      "27481 6871\n",
      "49 (27481, 40) (6871, 40)\n",
      "25044 6262\n",
      "50 (25044, 40) (6262, 40)\n"
     ]
    }
   ],
   "source": [
    "if cross_silo:\n",
    "    for index in range(0, len(split_dfs), 2):\n",
    "        train_state = split_dfs[index]\n",
    "        test_state = split_dfs[index + 1]\n",
    "        print(len(train_state), len(test_state))\n",
    "        (\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            train_groups,\n",
    "            train_second_groups,\n",
    "            train_third_groups,\n",
    "        ) = pre_process_single_datasets(train_state)\n",
    "        (\n",
    "            test_data,\n",
    "            test_labels,\n",
    "            test_groups,\n",
    "            test_second_groups,\n",
    "            test_third_groups,\n",
    "        ) = pre_process_single_datasets(test_state)\n",
    "\n",
    "        print(index // 2, train_data[0].shape, test_data[0].shape)\n",
    "\n",
    "        if not os.path.exists(\n",
    "            f\"{folder}FL_data/federated/{index // 2}\"\n",
    "        ):\n",
    "            os.makedirs(f\"{folder}FL_data/federated/{index // 2}\")\n",
    "            # save partitions_names \n",
    "        json_file = {index:data for index, data in enumerate(partitions_names)}\n",
    "        with open(f\"{folder}FL_data/federated/partitions_names.json\", \"w\") as f:\n",
    "            json.dump(json_file, f)\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_dataframes_{index // 2}_train.npy\",\n",
    "            train_data[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_labels_{index // 2}_train.npy\",\n",
    "            train_labels[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_groups_{index // 2}_train.npy\",\n",
    "            train_groups[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_second_groups_{index // 2}_train.npy\",\n",
    "            train_second_groups[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_third_groups_{index // 2}_train.npy\",\n",
    "            train_third_groups[0],\n",
    "        )\n",
    "\n",
    "\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_dataframes_{index // 2}_test.npy\",\n",
    "            test_data[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_labels_{index // 2}_test.npy\",\n",
    "            test_labels[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_groups_{index // 2}_test.npy\",\n",
    "            test_groups[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_second_groups_{index // 2}_test.npy\",\n",
    "            test_second_groups[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index // 2}/income_third_groups_{index // 2}_test.npy\",\n",
    "            test_third_groups[0],\n",
    "        )\n",
    "else:\n",
    "    for index in range(0, len(split_dfs)):\n",
    "        train_state = split_dfs[index]\n",
    "        (\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            train_groups,\n",
    "            train_second_groups,\n",
    "            train_third_groups,\n",
    "        ) = pre_process_single_datasets(train_state)\n",
    "\n",
    "        print(index, train_data[0].shape)\n",
    "\n",
    "        if not os.path.exists(\n",
    "            f\"{folder}FL_data/federated/{index}\"\n",
    "        ):\n",
    "            os.makedirs(f\"{folder}FL_data/federated/{index}\")\n",
    "            # save partitions_names \n",
    "        json_file = {index:data for index, data in enumerate(partitions_names)}\n",
    "        with open(f\"{folder}FL_data/federated/partitions_names.json\", \"w\") as f:\n",
    "            json.dump(json_file, f)\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index}/income_dataframes_{index}_train.npy\",\n",
    "            train_data[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index}/income_labels_{index}_train.npy\",\n",
    "            train_labels[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index}/income_groups_{index}_train.npy\",\n",
    "            train_groups[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index}/income_second_groups_{index}_train.npy\",\n",
    "            train_second_groups[0],\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{folder}FL_data/federated/{index}/income_third_groups_{index}_train.npy\",\n",
    "            train_third_groups[0],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25044, 40)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62025316, 1.        , 0.74389002, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.44303797, 2.        , 0.57026477, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.58227848, 1.        , 0.36761711, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.4556962 , 1.        , 0.33044807, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.3164557 , 1.        , 0.12321792, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.63291139, 1.        , 0.31364562, ..., 0.        , 1.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_dataframes_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_labels_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(f\"{folder}FL_data/federated/0/income_second_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "third = np.load(f\"{folder}FL_data/federated/0/income_third_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third = [item[0] for item in third]\n",
    "set(third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if cross_silo:\n",
    "    np.load(f\"{folder}FL_data/federated/0/income_second_groups_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if cross_silo:\n",
    "    np.load(f\"{folder}FL_data/federated/0/income_third_groups_0_train.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if cross_silo:\n",
    "    third = np.load(f\"{folder}FL_data/federated/0/income_third_groups_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if cross_silo:\n",
    "    np.load(f\"{folder}FL_data/federated/0/income_groups_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if cross_silo:\n",
    "    np.load(f\"{folder}FL_data/federated/0/income_labels_0_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All below is a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairfl_data.fairness_computation import _compute_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def equalized_odds_difference_by_outcome(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Calculates the equalized odds difference, considering outcomes separately for each group.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (np.array): True labels.\n",
    "    y_pred (np.array): Predicted labels.\n",
    "    sensitive_features (np.array): Sensitive feature(s) that define the groups.\n",
    "\n",
    "    Returns:\n",
    "    float: The equalized odds difference.\n",
    "    \"\"\"\n",
    "    if not isinstance(sensitive_features, pd.Series):\n",
    "        sensitive_features = pd.Series(sensitive_features)\n",
    "\n",
    "    unique_groups = sensitive_features.unique()\n",
    "    possible_outcomes = np.unique(y_true)\n",
    "\n",
    "    tpr_values = {group: {} for group in unique_groups}\n",
    "    fpr_values = {group: {} for group in unique_groups}\n",
    "\n",
    "    for group in unique_groups:\n",
    "        group_indices = sensitive_features[sensitive_features == group].index\n",
    "        y_true_group = y_true[group_indices]\n",
    "        y_pred_group = y_pred[group_indices]\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group).ravel()\n",
    "        tpr_values[group][1] = tp / (tp + fn) if (tp + fn) > 0 else 0  # TPR for outcome 1\n",
    "        fpr_values[group][1] = fp / (fp + tn) if (fp + tn) > 0 else 0  # FPR for outcome 1\n",
    "        # For binary classification where outcomes are 0 and 1, we can also consider metrics for the negative outcome (0)\n",
    "        if len(possible_outcomes) == 2:\n",
    "            tn_r, fn_r, fp_r, tp_r = confusion_matrix(1 - y_true_group, 1 - y_pred_group).ravel()\n",
    "            tpr_values[group][0] = tp_r / (tp_r + fp_r) if (tp_r + fp_r) > 0 else 0 # TNR (TPR for outcome 0)\n",
    "            fpr_values[group][0] = fn_r / (fn_r + tn_r) if (fn_r + tn_r) > 0 else 0 # FNR (FPR for outcome 0)\n",
    "\n",
    "    tpr_diffs = []\n",
    "    fpr_diffs = []\n",
    "\n",
    "    for outcome in possible_outcomes:\n",
    "        tprs_outcome = [tpr_values[group].get(outcome, 0) for group in unique_groups]\n",
    "        fprs_outcome = [fpr_values[group].get(outcome, 0) for group in unique_groups]\n",
    "\n",
    "        tpr_diffs.append(max(tprs_outcome) - min(tprs_outcome))\n",
    "        fpr_diffs.append(max(fprs_outcome) - min(fprs_outcome))\n",
    "\n",
    "    return max(abs(max(tpr_diffs)), abs(max(fpr_diffs)))\n",
    "\n",
    "y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1])\n",
    "sensitive_features = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Worst-case aggregation\n",
    "eo_diff_worst = equalized_odds_difference_by_outcome(y_true, y_pred, sensitive_features)\n",
    "print(f\"Equalized Odds Difference (by outcome, worst_case): {eo_diff_worst:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def equalized_odds_disparity_details(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Calculates the equalized odds difference and provides details on group-wise disparities.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (np.array): True labels.\n",
    "    y_pred (np.array): Predicted labels.\n",
    "    sensitive_features (np.array): Sensitive feature(s) that define the groups.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing:\n",
    "        - 'equalized_odds_difference': The overall equalized odds difference.\n",
    "        - 'group_metrics': TPR and FPR for each sensitive group and outcome.\n",
    "        - 'max_tpr_disparity': Details of the largest TPR difference.\n",
    "        - 'max_fpr_disparity': Details of the largest FPR difference.\n",
    "    \"\"\"\n",
    "    if not isinstance(sensitive_features, pd.Series):\n",
    "        sensitive_features = pd.Series(sensitive_features)\n",
    "\n",
    "    unique_groups = sorted(sensitive_features.unique())\n",
    "    possible_outcomes = np.unique(y_true)\n",
    "\n",
    "    group_metrics = {}\n",
    "    for group in unique_groups:\n",
    "        group_metrics[group] = {}\n",
    "        group_indices = sensitive_features[sensitive_features == group].index\n",
    "        y_true_group = y_true[group_indices]\n",
    "        y_pred_group = y_pred[group_indices]\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group).ravel()\n",
    "        group_metrics[group][1] = {'TPR': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "                                   'FPR': fp / (fp + tn) if (fp + tn) > 0 else 0}\n",
    "        if len(possible_outcomes) == 2:\n",
    "            tn_r, fn_r, fp_r, tp_r = confusion_matrix(1 - y_true_group, 1 - y_pred_group).ravel()\n",
    "            group_metrics[group][0] = {'TPR': tp_r / (tp_r + fp_r) if (tp_r + fp_r) > 0 else 0, # TNR\n",
    "                                       'FPR': fn_r / (fn_r + tn_r) if (fn_r + tn_r) > 0 else 0} # FNR\n",
    "\n",
    "    max_tpr_diff = 0\n",
    "    max_tpr_groups = None\n",
    "    max_tpr_outcome = None\n",
    "\n",
    "    max_fpr_diff = 0\n",
    "    max_fpr_groups = None\n",
    "    max_fpr_outcome = None\n",
    "\n",
    "    for outcome in possible_outcomes:\n",
    "        tprs = {group: group_metrics[group].get(outcome, {}).get('TPR', 0) for group in unique_groups}\n",
    "        fprs = {group: group_metrics[group].get(outcome, {}).get('FPR', 0) for group in unique_groups}\n",
    "\n",
    "        current_tpr_diff = max(tprs.values()) - min(tprs.values())\n",
    "        if abs(current_tpr_diff) > abs(max_tpr_diff):\n",
    "            max_tpr_diff = current_tpr_diff\n",
    "            min_tpr_group = [g for g, tpr in tprs.items() if tpr == min(tprs.values())]\n",
    "            max_tpr_group = [g for g, tpr in tprs.items() if tpr == max(tprs.values())]\n",
    "            max_tpr_groups = (min_tpr_group, max_tpr_group)\n",
    "            max_tpr_outcome = outcome\n",
    "\n",
    "        current_fpr_diff = max(fprs.values()) - min(fprs.values())\n",
    "        if abs(current_fpr_diff) > abs(max_fpr_diff):\n",
    "            max_fpr_diff = current_fpr_diff\n",
    "            min_fpr_group = [g for g, fpr in fprs.items() if fpr == min(fprs.values())]\n",
    "            max_fpr_group = [g for g, fpr in fprs.items() if fpr == max(fprs.values())]\n",
    "            max_fpr_groups = (min_fpr_group, max_fpr_group)\n",
    "            max_fpr_outcome = outcome\n",
    "\n",
    "    overall_eo_diff = max(abs(max_tpr_diff), abs(max_fpr_diff))\n",
    "\n",
    "    return {\n",
    "        'equalized_odds_difference': overall_eo_diff,\n",
    "        'group_metrics': group_metrics,\n",
    "        'max_tpr_disparity': {\n",
    "            'outcome': max_tpr_outcome,\n",
    "            'groups': max_tpr_groups,\n",
    "            'difference': max_tpr_diff\n",
    "        },\n",
    "        'max_fpr_disparity': {\n",
    "            'outcome': max_fpr_outcome,\n",
    "            'groups': max_fpr_groups,\n",
    "            'difference': max_fpr_diff\n",
    "        }\n",
    "    }\n",
    "\n",
    "y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1])\n",
    "sensitive_features = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "disparity_analysis = equalized_odds_disparity_details(y_true, y_pred, sensitive_features)\n",
    "print(f\"Equalized Odds Difference: {disparity_analysis['equalized_odds_difference']:.4f}\")\n",
    "print(\"\\nGroup-wise Metrics:\")\n",
    "print(disparity_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def demographic_parity_difference_by_outcome(y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Computes the demographic parity difference as:\n",
    "    max(P(Ŷ=y | A=group)) - min(P(Ŷ=y | A=group))\n",
    "    for each outcome y, and returns the maximum across outcomes,\n",
    "    along with the (outcome, group_max, group_min) responsible.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (np.array): Predicted labels (can be binary or multiclass).\n",
    "    sensitive_features (np.array or pd.Series): Sensitive feature(s) defining the groups.\n",
    "\n",
    "    Returns:\n",
    "    float: The maximum demographic parity difference across outcomes.\n",
    "    tuple: (outcome, group_max, group_min) responsible for this max difference.\n",
    "    \"\"\"\n",
    "    if not isinstance(sensitive_features, pd.Series):\n",
    "        sensitive_features = pd.Series(sensitive_features)\n",
    "\n",
    "    unique_groups = sensitive_features.unique()\n",
    "    unique_outcomes = np.unique(y_pred)\n",
    "\n",
    "    max_diff = 0.0\n",
    "    responsible_info = (None, None, None)  # (outcome, group_max, group_min)\n",
    "\n",
    "    for outcome in unique_outcomes:\n",
    "        outcome_probs = {}\n",
    "\n",
    "        for group in unique_groups:\n",
    "            group_indices = sensitive_features[sensitive_features == group].index\n",
    "            y_pred_group = y_pred[group_indices]\n",
    "            prob = np.mean(y_pred_group == outcome)\n",
    "            outcome_probs[group] = prob\n",
    "\n",
    "        group_max = max(outcome_probs, key=outcome_probs.get)\n",
    "        group_min = min(outcome_probs, key=outcome_probs.get)\n",
    "        diff = outcome_probs[group_max] - outcome_probs[group_min]\n",
    "\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "            responsible_info = (outcome, group_max, group_min)\n",
    "\n",
    "    return max_diff, responsible_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([1, 0, 1, 1, 0, 0, 1])\n",
    "sensitive = np.array(['A', 'A', 'B', 'B', 'B', 'C', 'C'])\n",
    "\n",
    "disparity, (group1, group2) = demographic_disparity_by_group(y_pred, sensitive)\n",
    "print(f\"Max demographic disparity: {disparity:.3f} between groups {group1} and {group2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate\n",
    "y_pred = np.array([1, 0, 1, 1, 0, 0, 1])\n",
    "sensitive = np.array(['A', 'A', 'B', 'B', 'B', 'C', 'C'])\n",
    "y_true = np.array([1, 0, 1, 1, 0, 0, 1])\n",
    "\n",
    "sf_data = pd.DataFrame(\n",
    "    {\n",
    "        \"DP_RACE\": sensitive\n",
    "    }\n",
    ")\n",
    "\n",
    "sel_rate = MetricFrame(\n",
    "    metrics={\"sel\":selection_rate},\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sf_data,\n",
    "    )\n",
    "df = sel_rate.by_group\n",
    "diff_matrix = df['sel'].values[:, None] - df['sel'].values[None, :]\n",
    "index = df.index.values\n",
    "column_names = [f\"{index[i]}_{index[j]}\" for i, j in product(range(len(df)), repeat=2)]\n",
    "\n",
    "diff_df = pd.Series(diff_matrix.flatten(), index=column_names)\n",
    "diff_df = pd.Series([diff_df.max(),diff_df.idxmax()], index=[f\"DP_SEX_DP\", f\"DP_SEX_val\"])\n",
    "diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df[1].split(\"_\")[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairFL-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
