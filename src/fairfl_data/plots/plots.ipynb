{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805c74c",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d537695",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_name = \"Test Node\"\n",
    "attributes = [\"- Third Disp.\", \"- Second Disp.\", \"- Disp.\", \"- Acc.\", \"- Group 3\"]\n",
    "\n",
    "mapping = {\n",
    "    \"- Third Disp.\": \"DP_RACE\",\n",
    "    \"- Second Disp.\": \"DP_MAR\",\n",
    "    \"- Disp.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_plot(\n",
    "    df: pd.DataFrame,\n",
    "    y_label: str,\n",
    "    title: str,\n",
    "    attribute: str,\n",
    "    font_size_labels: int = 22,\n",
    "    font_size_title: int = 22,\n",
    "    font_size_ticks: int = 22,\n",
    "    file_name: str = None,\n",
    "    save: bool = False\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Define a color-blind-friendly color\n",
    "    marker_face_color = '#56B4E9'  # Blue from ColorBrewer Set2\n",
    "    marker_edge_color = 'black'    # Black edges for visibility\n",
    "\n",
    "    for val in df['Value'].unique():\n",
    "        y_vals = df[df['Value'] == val][attribute]\n",
    "        x_vals = [int(float(val))] * len(y_vals)\n",
    "        plt.scatter(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            facecolors=marker_face_color,\n",
    "            edgecolors=marker_edge_color,\n",
    "            marker='o',\n",
    "            s=200,\n",
    "            linewidths=1.2\n",
    "        )\n",
    "\n",
    "    plt.xlabel('Sensitive Group Value', fontsize=font_size_labels)\n",
    "    plt.ylabel(y_label, fontsize=font_size_labels)\n",
    "    plt.title(title, fontsize=font_size_title)\n",
    "\n",
    "    max_val = int(float(max(df['Value'])))\n",
    "    labels = list(range(1, max_val + 1))\n",
    "    plt.xticks(ticks=labels, labels=labels, fontsize=font_size_ticks)\n",
    "    plt.yticks(fontsize=font_size_ticks)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f\"./images/{file_name}.pdf\", bbox_inches='tight', dpi=150)\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_differences(df1, df2):\n",
    "    # Set the 'State' column as index if not already\n",
    "    df1 = df1.set_index('dataset')\n",
    "    df2 = df2.set_index('dataset')\n",
    "\n",
    "    # Compute differences\n",
    "    diff_df = pd.DataFrame({\n",
    "        'DP_RACE': df1['DP_RACE'] - df2['DP_RACE'],\n",
    "        'DP_SEX': df1['DP_SEX'] - df2['DP_SEX'],\n",
    "    })\n",
    "    # Add dataset name metadata as columns\n",
    "    # df1 = df1.reset_index()\n",
    "    # diff_df[\"dataset\"] = df1[\"dataset\"]\n",
    "    # Optional: Reset index if you want 'State' as a column\n",
    "    # diff_df = diff_df.reset_index()\n",
    "    diff_df = diff_df.reset_index()\n",
    "    diff_df.head()\n",
    "    return diff_df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def plot_comparison_fairness(df: pd.DataFrame, title: str):\n",
    "    custom_palette = {\"DP_SEX\": \"#1E88E5\", \"DP_RACE\": \"#D81B60\"}\n",
    "\n",
    "    # 1) Count maximum unfairness and represent client unfairness per attribute\n",
    "    df['max_unfairness'] = df[['DP_SEX', 'DP_RACE']].max(axis=1)\n",
    "    unfairness_counts = df[['DP_SEX', 'DP_RACE']].apply(lambda row: row[row == row.max()].index.tolist(), axis=1)\n",
    "\n",
    "    # Flatten the list of lists and count occurrences\n",
    "    attribute_unfairness_counts = Counter([item for sublist in unfairness_counts for item in sublist])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(\n",
    "        x=list(attribute_unfairness_counts.keys()),\n",
    "        y=list(attribute_unfairness_counts.values()),\n",
    "        palette=custom_palette\n",
    "    )\n",
    "    plt.title(f'Number of Clients with Maximum Unfairness Towards Each Attribute - {title}')\n",
    "    plt.xlabel('Sensitive Attribute')\n",
    "    plt.ylabel('Number of Clients')\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Visualize clients unfair toward different groups\n",
    "    df_reset = df.reset_index()  # Make the index a regular column\n",
    "    df_melted = df_reset.melt(\n",
    "        id_vars='index',\n",
    "        value_vars=['DP_SEX', 'DP_RACE'],\n",
    "        var_name='Sensitive Attribute',\n",
    "        value_name='Unfairness Score'\n",
    "    )\n",
    "\n",
    "    ticks = list(range(0, min(50, len(df))))  # Adjusted for robustness\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x='index',\n",
    "        y='Unfairness Score',\n",
    "        hue='Sensitive Attribute',\n",
    "        data=df_melted,\n",
    "        palette=custom_palette\n",
    "    )\n",
    "    plt.title(f'Unfairness Score of Each Client Towards Different Sensitive Attributes - {title}')\n",
    "    plt.xlabel('Client Index')\n",
    "    plt.ylabel('Unfairness Score')\n",
    "    plt.legend(title='Sensitive Attribute')\n",
    "    plt.xticks(ticks, rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Cumulative Distribution Function of unfairness\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for col in ['DP_SEX', 'DP_RACE']:\n",
    "        sns.kdeplot(df[col], cumulative=True, label=col, color=custom_palette[col])\n",
    "\n",
    "    plt.title(f'Cumulative Distribution Function of Unfairness - {title}')\n",
    "    plt.xlabel('Unfairness Score')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.legend(title='Sensitive Attribute')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def bar_plot_differences(df, labels, title,\n",
    "                                        font_size_title=25, font_size_ticks=22, font_size_labels=24, y_axis=\"\",\n",
    "                                        save: bool = False, fig_name: str = \"\", legend_name: str = \"bar_plot_differences_legend\"):\n",
    "    \"\"\"\n",
    "    Creates a grouped bar plot of unfairness scores for different sensitive attributes,\n",
    "    with the legend saved as a separate image.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'State' as index and 'DP_SEX', 'DP_RACE' columns.\n",
    "        labels (list): List of state labels for the x-axis.\n",
    "        title (str): Title of the plot.\n",
    "        font_size_title (int, optional): Font size for the title. Defaults to 25.\n",
    "        font_size_ticks (int, optional): Font size for the axis ticks. Defaults to 22.\n",
    "        font_size_labels (int, optional): Font size for the axis labels. Defaults to 24.\n",
    "        y_axis (str, optional): Label for the y-axis. Defaults to \"\".\n",
    "        save (bool, optional): Whether to save the plot. Defaults to False.\n",
    "        fig_name (str, optional): Filename for saving the plot. Defaults to \"\".\n",
    "        legend_name (str, optional): Filename for saving the legend. Defaults to \"bar_plot_differences_legend\".\n",
    "    \"\"\"\n",
    "    df_reset = df.reset_index().rename(columns={'index': 'State'})\n",
    "    df_melted = df_reset.melt(id_vars='State',\n",
    "                              value_vars=['DP_SEX', 'DP_RACE'],\n",
    "                              var_name='Sensitive Attribute',\n",
    "                              value_name='Unfairness Score')\n",
    "\n",
    "    ticks = list(range(len(labels)))\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))  # Create the main figure and axes\n",
    "\n",
    "    # Define custom colors\n",
    "    custom_palette = {\"DP_SEX\": \"#1E88E5\", \"DP_RACE\": \"#D81B60\"}\n",
    "\n",
    "    sns.barplot(\n",
    "        x='State', y='Unfairness Score',\n",
    "        hue='Sensitive Attribute', data=df_melted,\n",
    "        palette=custom_palette, ax=ax  # Pass the axes to seaborn\n",
    "    )\n",
    "\n",
    "    # Title and labels\n",
    "    ax.set_title(title, fontsize=font_size_title, pad=20)\n",
    "    ax.set_xlabel('State', fontsize=font_size_labels, labelpad=15)\n",
    "    ax.set_ylabel(y_axis, fontsize=font_size_labels, labelpad=15)\n",
    "\n",
    "    # Ticks\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=90, ha='right', fontsize=font_size_ticks)\n",
    "    ax.tick_params(axis='y', labelsize=font_size_ticks)\n",
    "\n",
    "    # Get the legend object\n",
    "    legend = ax.get_legend()\n",
    "    if legend:\n",
    "        # Remove the legend from the main plot\n",
    "        legend.remove()\n",
    "\n",
    "        if save:\n",
    "            # Create a separate figure for the legend\n",
    "            fig_legend = plt.figure(figsize=(6, 1))  # Adjust size as needed\n",
    "            ax_legend = fig_legend.add_subplot(111)\n",
    "            # Re-draw the legend on the separate figure\n",
    "            handles, labels_legend = ax.get_legend_handles_labels()\n",
    "            ax_legend.legend(handles, labels_legend,\n",
    "                               loc='center',\n",
    "                               fontsize=font_size_ticks,\n",
    "                               ncol=2, frameon=False)\n",
    "            ax_legend.axis('off')\n",
    "            fig_legend.tight_layout()\n",
    "            plt.savefig(f\"./images/{legend_name}.pdf\", bbox_inches='tight', dpi=150)\n",
    "            plt.close(fig_legend)\n",
    "\n",
    "        # Save the main plot\n",
    "        if save:\n",
    "            plt.savefig(f\"./images/{fig_name}.pdf\", bbox_inches='tight', dpi=150)\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Save the main plot even if there's no legend\n",
    "        if save:\n",
    "            plt.savefig(f\"./images/{fig_name}.pdf\", bbox_inches='tight', dpi=150)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def scatter_fairness_plot(\n",
    "#     df1: pd.DataFrame,\n",
    "#     client_column: str = \"Partition ID\",\n",
    "#     fairness_column_Y: str = \"RAC1P_DP\",\n",
    "#     fairness_column_X: str = \"RAC1P_DP\",\n",
    "#     title: str = \"Fairness Before/After Comparison\",\n",
    "#     figsize: tuple = (6, 6),\n",
    "#     ylabel: str = \"Fairness Value Before\",\n",
    "#     xlabel: str = \"Fairness Value After\",\n",
    "#     title_font_size: int = 25,\n",
    "#     label_font_size: int = 25,\n",
    "#     ticks_font_size: int = 20,\n",
    "# ) -> plt.Figure:\n",
    "#     \"\"\"\n",
    "#     Plot a scatter comparison of two fairness metrics from the same DataFrame.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df1 : pd.DataFrame\n",
    "#         DataFrame containing client IDs and two fairness metrics.\n",
    "\n",
    "#     client_column : str, default=\"Partition ID\"\n",
    "#         Name of the column containing the client IDs.\n",
    "\n",
    "#     fairness_column_Y : str, default=\"RAC1P_DP\"\n",
    "#         Column for y-axis values.\n",
    "\n",
    "#     fairness_column_X : str, default=\"RAC1P_DP\"\n",
    "#         Column for x-axis values.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     matplotlib.figure.Figure\n",
    "#         The matplotlib figure object containing the plot.\n",
    "#     \"\"\"\n",
    "#     assert df1[client_column].is_unique, \"The client ID column must be unique.\"\n",
    "\n",
    "#     fairness1 = df1[fairness_column_X]\n",
    "#     fairness2 = df1[fairness_column_Y]\n",
    "\n",
    "#     min_val = min(fairness1.min(), fairness2.min())\n",
    "#     max_val = max(fairness1.max(), fairness2.max())\n",
    "#     padding = 0.05 * (max_val - min_val)\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "#     # Use color-blind friendly palette\n",
    "#     marker_face_color = '#56B4E9'  # Green tone\n",
    "#     marker_edge_color = 'black'\n",
    "\n",
    "#     ax.scatter(\n",
    "#         fairness1,\n",
    "#         fairness2,\n",
    "#         facecolors=marker_face_color,\n",
    "#         edgecolors=marker_edge_color,\n",
    "#         marker='o',\n",
    "#         s=200,\n",
    "#         linewidths=1.2,\n",
    "#         alpha=0.8\n",
    "#     )\n",
    "\n",
    "#     ax.plot(\n",
    "#         [min_val - padding, max_val + padding],\n",
    "#         [min_val - padding, max_val + padding],\n",
    "#         linestyle=\"dotted\",\n",
    "#         color=\"gray\",\n",
    "#         linewidth=2\n",
    "#     )\n",
    "\n",
    "#     ax.tick_params(axis=\"both\", which=\"major\", labelsize=ticks_font_size)\n",
    "#     ax.set_xlim(min_val - padding, max_val + padding)\n",
    "#     ax.set_ylim(min_val - padding, max_val + padding)\n",
    "#     ax.set_xlabel(xlabel, fontsize=label_font_size)\n",
    "#     ax.set_ylabel(ylabel, fontsize=label_font_size)\n",
    "#     ax.set_title(title, fontsize=title_font_size)\n",
    "#     ax.grid(True)\n",
    "\n",
    "#     return fig\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def scatter_fairness_plot(\n",
    "    df1: pd.DataFrame,\n",
    "    client_column: str = \"Partition ID\",\n",
    "    fairness_column_Y: str = \"RAC1P_DP\",\n",
    "    fairness_column_X: str = \"RAC1P_DP\",\n",
    "    title: str = \"Fairness Comparison\",\n",
    "    figsize: tuple = (6, 6),\n",
    "    ylabel: str = \"Fairness Metric Y\",\n",
    "    xlabel: str = \"Fairness Metric X\",\n",
    "    title_font_size: int = 25,\n",
    "    label_font_size: int = 25,\n",
    "    ticks_font_size: int = 20,\n",
    "    unfairness_distribution: dict = None,\n",
    "    legend_filename: str = \"fairness_plot_legend.png\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot a scatter comparison of two fairness metrics from the same DataFrame,\n",
    "    coloring points based on state lists, with the legend saved separately.\n",
    "    \"\"\"\n",
    "    assert df1[client_column].is_unique, \"The client ID column must be unique.\"\n",
    "\n",
    "    fairness_x = df1[fairness_column_X]\n",
    "    fairness_y = df1[fairness_column_Y]\n",
    "    clients = df1[client_column]\n",
    "\n",
    "    min_val = min(fairness_x.min(), fairness_y.min())\n",
    "    max_val = max(fairness_x.max(), fairness_y.max())\n",
    "    padding = 0.05 * (max_val - min_val)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "\n",
    "    # Define colors for the two groups\n",
    "    race_color = '#D81B60'  # Vivid orange\n",
    "    sex_color = '#1E88E5'   # Blue\n",
    "\n",
    "    # Lists to store data for each group\n",
    "    race_x = []\n",
    "    race_y = []\n",
    "    sex_x = []\n",
    "    sex_y = []\n",
    "\n",
    "    if unfairness_distribution:\n",
    "        race_states = unfairness_distribution.get(\"race_state\", [])\n",
    "        sex_states = unfairness_distribution.get(\"sex_states\", [])\n",
    "\n",
    "        for i, client in enumerate(clients):\n",
    "            if client in race_states:\n",
    "                race_x.append(fairness_x.iloc[i])\n",
    "                race_y.append(fairness_y.iloc[i])\n",
    "            elif client in sex_states:\n",
    "                sex_x.append(fairness_x.iloc[i])\n",
    "                sex_y.append(fairness_y.iloc[i])\n",
    "            else:\n",
    "                # We are not including the 'other' states\n",
    "                pass\n",
    "    else:\n",
    "        # If no unfairness_distribution is provided, plot all points with a default color\n",
    "        ax.scatter(\n",
    "            fairness_x,\n",
    "            fairness_y,\n",
    "            facecolors='#56B4E9',\n",
    "            edgecolors='black',\n",
    "            marker='o',\n",
    "            s=200,\n",
    "            linewidths=1.2,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        ax.plot(\n",
    "            [min_val - padding, max_val + padding],\n",
    "            [min_val - padding, max_val + padding],\n",
    "            linestyle=\"dotted\",\n",
    "            color=\"gray\",\n",
    "            linewidth=2\n",
    "        )\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=ticks_font_size)\n",
    "        ax.set_xlim(min_val - padding, max_val + padding)\n",
    "        ax.set_ylim(min_val - padding, max_val + padding)\n",
    "        ax.set_xlabel(xlabel, fontsize=label_font_size)\n",
    "        ax.set_ylabel(ylabel, fontsize=label_font_size)\n",
    "        ax.set_title(title, fontsize=title_font_size)\n",
    "        ax.grid(True)\n",
    "        fig.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    # Scatter plot for race-related states\n",
    "    ax.scatter(\n",
    "        race_x,\n",
    "        race_y,\n",
    "        facecolors=race_color,\n",
    "        edgecolors='black',\n",
    "        marker='o',\n",
    "        s=200,\n",
    "        linewidths=1.2,\n",
    "        alpha=0.8,\n",
    "        label='Race-Related States'\n",
    "    )\n",
    "\n",
    "    # Scatter plot for sex-related states\n",
    "    ax.scatter(\n",
    "        sex_x,\n",
    "        sex_y,\n",
    "        facecolors=sex_color,\n",
    "        edgecolors='black',\n",
    "        marker='o',\n",
    "        s=200,\n",
    "        linewidths=1.2,\n",
    "        alpha=0.8,\n",
    "        label='Sex-Related States'\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        [min_val - padding, max_val + padding],\n",
    "        [min_val - padding, max_val + padding],\n",
    "        linestyle=\"dotted\",\n",
    "        color=\"gray\",\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=ticks_font_size)\n",
    "    ax.set_xlim(min_val - padding, max_val + padding)\n",
    "    ax.set_ylim(min_val - padding, max_val + padding)\n",
    "    ax.set_xlabel(xlabel, fontsize=label_font_size)\n",
    "    ax.set_ylabel(ylabel, fontsize=label_font_size)\n",
    "    ax.set_title(title, fontsize=title_font_size)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend = plt.figure(figsize=(6, 1))\n",
    "    ax_legend = fig_legend.add_subplot(111)\n",
    "\n",
    "    # Create custom patches for the legend\n",
    "    race_patch = mpatches.Patch(facecolor=race_color, edgecolor='black', label='Race-Related States')\n",
    "    sex_patch = mpatches.Patch(facecolor=sex_color, edgecolor='black', label='Sex-Related States')\n",
    "\n",
    "    # Add the legend to the separate axes\n",
    "    ax_legend.legend(handles=[race_patch, sex_patch], fontsize=24, loc='center', frameon=False, ncol=2)\n",
    "    ax_legend.axis('off')  # Turn off the axes for the legend\n",
    "\n",
    "    fig_legend.tight_layout()\n",
    "    fig_legend.savefig(legend_filename)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def local_client_fairness_plot(\n",
    "#     df1: pd.DataFrame,\n",
    "#     df2: pd.DataFrame,\n",
    "#     client_column: str = \"Partition ID\",\n",
    "#     fairness_column: str = \"RAC1P_DP\",\n",
    "#     title: str = \"Fairness Before/After Comparison\",\n",
    "#     figsize: tuple = (6, 6),\n",
    "#     ylabel: str = \"Fairness Value Before\",\n",
    "#     xlabel: str = \"Fairness Value After\",\n",
    "#     title_font_size: int = 25,\n",
    "#     label_font_size: int = 25,\n",
    "#     ticks_font_size: int = 20,\n",
    "#     unfairness_distribution: dict = None,\n",
    "    \n",
    "# ) -> plt.Figure:\n",
    "#     \"\"\"\n",
    "#     Plot a scatter comparison of fairness values from two dataframes.\n",
    "#     \"\"\"\n",
    "#     assert df1[client_column].is_unique, \"The client ID column must be unique.\"\n",
    "\n",
    "#     merged = pd.merge(\n",
    "#         df1[[client_column, fairness_column]].rename(columns={fairness_column: \"fairness1\"}),\n",
    "#         df2[[client_column, fairness_column]].rename(columns={fairness_column: \"fairness2\"}),\n",
    "#         on=client_column,\n",
    "#     )\n",
    "\n",
    "#     fairness1 = merged[\"fairness1\"]\n",
    "#     fairness2 = merged[\"fairness2\"]\n",
    "#     min_val = min(fairness1.min(), fairness2.min())\n",
    "#     max_val = max(fairness1.max(), fairness2.max())\n",
    "#     padding = 0.05 * (max_val - min_val)\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "\n",
    "#     # Color-blind-friendly style\n",
    "#     marker_face_color = '#56B4E9'  # Orange from color-blind safe palette\n",
    "#     marker_edge_color = 'black'\n",
    "\n",
    "\n",
    "#     ax.scatter(\n",
    "#         fairness2,\n",
    "#         fairness1,\n",
    "#         facecolors=marker_face_color,\n",
    "#         edgecolors=marker_edge_color,\n",
    "#         marker='o',\n",
    "#         s=200,\n",
    "#         linewidths=1.2,\n",
    "#         alpha=0.8\n",
    "#     )\n",
    "\n",
    "#     ax.plot(\n",
    "#         [min_val - padding, max_val + padding],\n",
    "#         [min_val - padding, max_val + padding],\n",
    "#         linestyle=\"dotted\",\n",
    "#         color=\"gray\",\n",
    "#         linewidth=2\n",
    "#     )\n",
    "    \n",
    "#     ax.tick_params(axis=\"both\", which=\"major\", labelsize=ticks_font_size)\n",
    "#     ax.set_xlim(min_val - padding, max_val + padding)\n",
    "#     ax.set_ylim(min_val - padding, max_val + padding)\n",
    "#     ax.set_xlabel(xlabel, fontsize=label_font_size)\n",
    "#     ax.set_ylabel(ylabel, fontsize=label_font_size)\n",
    "#     ax.set_title(title, fontsize=title_font_size)\n",
    "#     ax.grid(True)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "\n",
    "#     return fig\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def local_client_fairness_plot(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    client_column: str = \"Partition ID\",\n",
    "    fairness_column: str = \"RAC1P_DP\",\n",
    "    title: str = \"Fairness Before/After Comparison\",\n",
    "    figsize: tuple = (6, 6),\n",
    "    ylabel: str = \"Fairness Value Before\",\n",
    "    xlabel: str = \"Fairness Value After\",\n",
    "    title_font_size: int = 25,\n",
    "    label_font_size: int = 25,\n",
    "    ticks_font_size: int = 20,\n",
    "    unfairness_distribution: dict = None,\n",
    "\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot a scatter comparison of fairness values from two dataframes,\n",
    "    coloring points based on state lists.\n",
    "    \"\"\"\n",
    "    assert df1[client_column].is_unique, \"The client ID column must be unique.\"\n",
    "\n",
    "    merged = pd.merge(\n",
    "        df1[[client_column, fairness_column]].rename(columns={fairness_column: \"fairness1\"}),\n",
    "        df2[[client_column, fairness_column]].rename(columns={fairness_column: \"fairness2\"}),\n",
    "        on=client_column,\n",
    "    )\n",
    "    print(merged.shape)\n",
    "\n",
    "    fairness1 = merged[\"fairness1\"]\n",
    "    fairness2 = merged[\"fairness2\"]\n",
    "    clients = merged[client_column]\n",
    "    min_val = min(fairness1.min(), fairness2.min())\n",
    "    max_val = max(fairness1.max(), fairness2.max())\n",
    "    padding = 0.05 * (max_val - min_val)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    # Define colors for the two groups\n",
    "    if unfairness_distribution:\n",
    "    \n",
    "        race_color = '#D81B60'  # Vivid orange\n",
    "        sex_color = '#1E88E5'   # Blue\n",
    "    else:\n",
    "        race_color = '#56B4E9'  # Vivid orange\n",
    "        sex_color = '#56B4E9'   # Blue\n",
    "    # Lists to store data for each group\n",
    "    race_x = []\n",
    "    race_y = []\n",
    "    sex_x = []\n",
    "    sex_y = []\n",
    "    other_x = []\n",
    "    other_y = []\n",
    "    other_labels = []\n",
    "\n",
    "\n",
    "    \n",
    "    if unfairness_distribution:\n",
    "        race_states = unfairness_distribution.get(\"race_state\", [])\n",
    "        sex_states = unfairness_distribution.get(\"sex_states\", [])\n",
    "        for i, client in enumerate(clients):\n",
    "            if client in race_states:\n",
    "                race_x.append(fairness2[i])\n",
    "                race_y.append(fairness1[i])\n",
    "            elif client in sex_states:\n",
    "                sex_x.append(fairness2[i])\n",
    "                sex_y.append(fairness1[i])\n",
    "    else:\n",
    "        for i, client in enumerate(clients):\n",
    "            race_x.append(fairness2[i])\n",
    "            race_y.append(fairness1[i])\n",
    "        # other_labels.append(client)\n",
    "\n",
    "\n",
    "    # Scatter plot for race-related states\n",
    "    ax.scatter(\n",
    "        race_x,\n",
    "        race_y,\n",
    "        facecolors=race_color,\n",
    "        edgecolors='black',\n",
    "        marker='o',\n",
    "        s=200,\n",
    "        linewidths=1.2,\n",
    "        alpha=0.8,\n",
    "        label='Race-Related States'\n",
    "    )\n",
    "\n",
    "    # Scatter plot for sex-related states\n",
    "    ax.scatter(\n",
    "        sex_x,\n",
    "        sex_y,\n",
    "        facecolors=sex_color,\n",
    "        edgecolors='black',\n",
    "        marker='o',\n",
    "        s=200,\n",
    "        linewidths=1.2,\n",
    "        alpha=0.8,\n",
    "        label='Sex-Related States'\n",
    "    )\n",
    "\n",
    "    # Scatter plot for other states (gray)\n",
    "    # ax.scatter(\n",
    "    #     other_x,\n",
    "    #     other_y,\n",
    "    #     facecolors='gray',\n",
    "    #     edgecolors='black',\n",
    "    #     marker='o',\n",
    "    #     s=200,\n",
    "    #     linewidths=1.2,\n",
    "    #     alpha=0.6,\n",
    "    #     label='Other States'\n",
    "    # )\n",
    "\n",
    "    ax.plot(\n",
    "        [min_val - padding, max_val + padding],\n",
    "        [min_val - padding, max_val + padding],\n",
    "        linestyle=\"dotted\",\n",
    "        color=\"gray\",\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=ticks_font_size)\n",
    "    ax.set_xlim(min_val - padding, max_val + padding)\n",
    "    ax.set_ylim(min_val - padding, max_val + padding)\n",
    "    ax.set_xlabel(xlabel, fontsize=label_font_size)\n",
    "    ax.set_ylabel(ylabel, fontsize=label_font_size)\n",
    "    ax.set_title(title, fontsize=title_font_size)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend = plt.figure(figsize=(6, 1))\n",
    "    ax_legend = fig_legend.add_subplot(111)\n",
    "\n",
    "    # Create custom patches for the legend\n",
    "    race_patch = mpatches.Patch(facecolor=race_color, edgecolor='black', label='Race-Biased State')\n",
    "    sex_patch = mpatches.Patch(facecolor=sex_color, edgecolor='black', label='Sex-Biased State')\n",
    "\n",
    "    # Add the legend to the separate axes\n",
    "    ax_legend.legend(handles=[race_patch, sex_patch], fontsize=24, loc='center', frameon=False, ncol=2)\n",
    "    ax_legend.axis('off')  # Turn off the axes for the legend\n",
    "\n",
    "    fig_legend.tight_layout()\n",
    "    fig_legend.savefig(\"./images/legend_blue_red.pdf\", bbox_inches='tight', dpi=150)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def visualize_value_change(df1: pd.DataFrame, df2: pd.DataFrame, sensitive_col: str = \"DP_RACE\", value_col: str = \"Value\", font_size: int = 26, ticks_font_size=20, title:str = \"\", y_label:str=\"\", initial_state:str=\"\"):\n",
    "    \"\"\"\n",
    "    Visualizes the change in a specified sensitive column across different value\n",
    "    categories between two dataframes, with refined arrow styling.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): The first dataframe representing the initial state.\n",
    "        df2 (pd.DataFrame): The second dataframe representing the final state.\n",
    "        sensitive_col (str, optional): The name of the sensitive column to track.\n",
    "                                       Defaults to \"DP_RACE\".\n",
    "        value_col (str, optional): The name of the column representing the value categories.\n",
    "                                   Defaults to \"Value\".\n",
    "    \"\"\"\n",
    "    # Merge the two dataframes based on the common columns\n",
    "    merged_df = pd.merge(df1, df2, on=\"dataset\", suffixes=('_df1', '_df2'))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Iterate and draw arrows first\n",
    "    arrow_head_width = 0.05\n",
    "    arrow_head_length = 0.05\n",
    "    arrow_alpha = 0.7\n",
    "    arrow_linewidth = 1.5\n",
    "    arrow_color = 'gray'\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        x1 = float(row[f'{value_col}_df1'])\n",
    "        y1 = float(row[f'{sensitive_col}_df1'])\n",
    "        x2 = float(row[f'{value_col}_df2'])\n",
    "        y2 = float(row[f'{sensitive_col}_df2'])\n",
    "\n",
    "        plt.arrow(x1, y1 , x2 - x1, y2 - y1,\n",
    "                  head_width=arrow_head_width,\n",
    "                  head_length=arrow_head_length,\n",
    "                  fc=arrow_color,\n",
    "                  ec=arrow_color,\n",
    "                  alpha=arrow_alpha,\n",
    "                  linewidth=arrow_linewidth,\n",
    "                  length_includes_head=True,\n",
    "                  zorder=1) # Ensure arrows are behind the points\n",
    "\n",
    "    # Styling for the initial state scatter plot\n",
    "    marker_face_color = '#004D40' \n",
    "    marker_edge_color = 'black' \n",
    "    marker_size = 200\n",
    "    marker_linewidth = 1.2\n",
    "\n",
    "    # Plot the initial state with the desired style\n",
    "    for val in df1[value_col].unique():\n",
    "        y_vals = df1[df1[value_col] == val][sensitive_col]\n",
    "        x_vals = [int(float(val))] * len(y_vals)\n",
    "        plt.scatter(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            facecolors=marker_face_color,\n",
    "            edgecolors=marker_edge_color,\n",
    "            marker='o',\n",
    "            s=marker_size,\n",
    "            linewidths=marker_linewidth,\n",
    "            label=initial_state if val == df1[value_col].unique()[0] else \"\", # Label only once\n",
    "            zorder=2, # Ensure initial state points are on top of arrows,\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "    # Plot the final state points on top of the arrows\n",
    "    for val in df2[value_col].unique():\n",
    "        y_vals = df2[df2[value_col] == val][sensitive_col]\n",
    "        x_vals = [int(float(val))] * len(y_vals)\n",
    "        plt.scatter(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            s=200,\n",
    "            color='#FFC107',\n",
    "            edgecolor='black',\n",
    "            label='FedAVG' if val == df2[value_col].unique()[0] else \"\", # Label only once\n",
    "            zorder=2, # Ensure final state points are on top of arrows\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "\n",
    "    # Customize the x-axis ticks based on the range in df1\n",
    "    max_val_df1 = int(float(df1[value_col].max()))\n",
    "    labels = list(range(1, max_val_df1 + 1))\n",
    "    plt.xticks(ticks=labels, labels=labels, fontsize=ticks_font_size)\n",
    "    plt.yticks(fontsize=ticks_font_size)\n",
    "    plt.xlabel('Sensitive Group Value', fontsize=font_size)\n",
    "    plt.ylabel(y_label, fontsize=font_size)\n",
    "    plt.title(title, fontsize=font_size)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # # Move legend outside plot, below\n",
    "    # ax.legend(loc='upper center',\n",
    "    #           bbox_to_anchor=(0.5, -0.15),\n",
    "    #           fontsize=20,\n",
    "    #           title_fontsize=22,\n",
    "    #           ncol=2, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_value_change(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    sensitive_col: str = \"DP_RACE\",\n",
    "    value_col: str = \"Value\",\n",
    "    font_size: int = 26,\n",
    "    ticks_font_size: int = 20,\n",
    "    \n",
    "    title: str = \"\",\n",
    "    y_label: str = \"\",\n",
    "    initial_state: str = \"\",\n",
    "    jitter_amount: float = 0.1,\n",
    "    legend_filename: str = \"value_change_legend.pdf\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualizes the change in a specified sensitive column across different value\n",
    "    categories between two dataframes with jittered dots and a separate legend file.\n",
    "    Arrows are drawn only if the 'Value' is different between the two connected states.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): The first dataframe representing the initial state.\n",
    "        df2 (pd.DataFrame): The second dataframe representing the final state.\n",
    "        sensitive_col (str, optional): The name of the sensitive column to track.\n",
    "                                       Defaults to \"DP_RACE\".\n",
    "        value_col (str, optional): The name of the column representing the value categories.\n",
    "                                   Defaults to \"Value\".\n",
    "        font_size (int, optional): Font size for labels and title. Defaults to 26.\n",
    "        ticks_font_size (int, optional): Font size for axis ticks. Defaults to 20.\n",
    "        title (str, optional): The title of the plot. Defaults to \"\".\n",
    "        y_label (str, optional): The label for the y-axis. Defaults to \"\".\n",
    "        initial_state (str, optional): Label for the initial state in the legend. Defaults to \"\".\n",
    "        jitter_amount (float, optional): The maximum horizontal shift for the dots.\n",
    "                                         Defaults to 0.1.\n",
    "        legend_filename (str, optional): The filename to save the legend.\n",
    "                                          Defaults to \"value_change_legend.png\".\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The matplotlib figure object for the main plot.\n",
    "    \"\"\"\n",
    "    # Merge the two dataframes based on the common columns\n",
    "    merged_df = pd.merge(df1, df2, on=\"dataset\", suffixes=('_df1', '_df2'))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Styling for the initial state scatter plot\n",
    "    marker_face_color = '#004D40'\n",
    "    marker_edge_color = 'black'\n",
    "    marker_size = 200\n",
    "    marker_linewidth = 1.2\n",
    "\n",
    "    # Arrow styling\n",
    "    arrow_head_width = 0.05\n",
    "    arrow_head_length = 0.05\n",
    "    arrow_alpha = 0.7\n",
    "    arrow_linewidth = 1.5\n",
    "    arrow_color = 'gray'\n",
    "\n",
    "    # Store jittered positions\n",
    "    jittered_positions_df1 = {}\n",
    "    jittered_positions_df2 = {}\n",
    "\n",
    "    # Plot the initial state with jitter\n",
    "    for val in sorted(df1[value_col].unique()):\n",
    "        subset = df1[df1[value_col] == val]\n",
    "        y_vals = subset[sensitive_col].astype(float)\n",
    "        x_base = int(float(val))\n",
    "        jitter = np.random.uniform(-jitter_amount, jitter_amount, len(y_vals))\n",
    "        x_vals = [x_base + j for j in jitter]\n",
    "        jittered_positions_df1[int(float(val))] = list(zip(subset['dataset'].values, x_vals, y_vals.values))\n",
    "        plt.scatter(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            facecolors=marker_face_color,\n",
    "            edgecolors=marker_edge_color,\n",
    "            marker='o',\n",
    "            s=marker_size,\n",
    "            linewidths=marker_linewidth,\n",
    "            label=initial_state if val == df1[value_col].unique()[0] else \"\",  # Label only once\n",
    "            zorder=2,  # Ensure initial state points are on top of arrows,\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "    # Plot the final state points with jitter\n",
    "    for val in sorted(df2[value_col].unique()):\n",
    "        \n",
    "        subset = df2[df2[value_col] == val]\n",
    "        y_vals = subset[sensitive_col].astype(float)\n",
    "        x_base = int(float(val))\n",
    "        jitter = np.random.uniform(-jitter_amount, jitter_amount, len(y_vals))\n",
    "        x_vals = [x_base + j for j in jitter]\n",
    "        jittered_positions_df2[int(float(val))] = list(zip(subset['dataset'].values, x_vals, y_vals.values))\n",
    "        plt.scatter(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            s=200,\n",
    "            color='#FFC107',\n",
    "            edgecolor='black',\n",
    "            label='FedAVG' if val == df2[value_col].unique()[0] else \"\",  # Label only once\n",
    "            zorder=2,  # Ensure final state points are on top of arrows\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "    # Draw arrows based on the 'dataset' identifier and different 'Value'\n",
    "    for index, row in merged_df.iterrows():\n",
    "        dataset_id = row['dataset']\n",
    "        val1_orig = str(row[f'{value_col}_df1'])\n",
    "        val2_orig = str(row[f'{value_col}_df2'])\n",
    "        # if int(val1_orig) != int(val2_orig):\n",
    "        #     print(val1_orig, val2_orig)\n",
    "        #     print(2 in jittered_positions_df1)\n",
    "        #     print(jittered_positions_df2)\n",
    "        \n",
    "        if int(float(val1_orig)) in jittered_positions_df1 and int(float(val2_orig)) in jittered_positions_df2 and val1_orig != val2_orig:\n",
    "            initial_matches = [match for match in jittered_positions_df1[int(float(val1_orig))] if match[0] == dataset_id]\n",
    "            final_matches = [match for match in jittered_positions_df2[int(float(val2_orig))] if match[0] == dataset_id]\n",
    "\n",
    "            if initial_matches and final_matches:\n",
    "                initial_x, initial_y = initial_matches[0][1], initial_matches[0][2]\n",
    "                final_x, final_y = final_matches[0][1], final_matches[0][2]\n",
    "                plt.arrow(initial_x, initial_y, final_x - initial_x, final_y - initial_y,\n",
    "                          head_width=arrow_head_width,\n",
    "                          head_length=arrow_head_length,\n",
    "                          fc=arrow_color,\n",
    "                          ec=arrow_color,\n",
    "                          alpha=arrow_alpha,\n",
    "                          linewidth=arrow_linewidth,\n",
    "                          length_includes_head=True,\n",
    "                          zorder=1)\n",
    "\n",
    "                print(\"Arrow\")\n",
    "\n",
    "    # Customize the x-axis ticks based on the range in df1\n",
    "    max_val_df1 = int(float(df1[value_col].max()))\n",
    "    labels = list(range(1, max_val_df1 + 1))\n",
    "    plt.xticks(ticks=labels, labels=labels, fontsize=ticks_font_size)\n",
    "    plt.yticks(fontsize=ticks_font_size)\n",
    "    plt.xlabel('Sensitive Group Value', fontsize=font_size)\n",
    "    plt.ylabel(y_label, fontsize=font_size)\n",
    "    plt.title(title, fontsize=font_size)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Create a separate figure for the legend\n",
    "    fig_legend = plt.figure(figsize=(6, 1))  # Adjust figure size as needed\n",
    "    ax_legend = fig_legend.add_subplot(111)\n",
    "\n",
    "    # Create custom patches for the legend\n",
    "    initial_patch = mpatches.Patch(facecolor='#004D40', edgecolor='black', alpha=0.6, label=initial_state)\n",
    "    fedavg_patch = mpatches.Patch(facecolor='#FFC107', edgecolor='black', alpha=0.8, label='FedAVG')\n",
    "\n",
    "    # Add the legend to the separate axes\n",
    "    ax_legend.legend(handles=[initial_patch, fedavg_patch], fontsize=20, loc='center', frameon=False, ncol=2)\n",
    "    ax_legend.axis('off')  # Turn off the axes for the legend\n",
    "\n",
    "    fig_legend.tight_layout()\n",
    "    fig_legend.savefig(legend_filename)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed75e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af7124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6a938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9febb3c",
   "metadata": {},
   "source": [
    "# Cross Silo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406dcdad",
   "metadata": {},
   "source": [
    "## VALUE Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_value_cross_silo\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_silo_value_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/hhqyjzsl\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5041243",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 51):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)].split(\"_\")[0]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90de4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Value\"] = [int(i) for i in results_df[\"Value\"]]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_sex_centralized = dp[dp[\"model\"] == \"LogisticRegression\"][[\"DP_RACE\", \"dataset\"]]\n",
    "# results_fedavg = results_df[[\"DP_RACE\", \"dataset\"]]\n",
    "\n",
    "# # Merge the two DataFrames on the \"dataset\" column\n",
    "# merged_df = pd.merge(dp_sex_centralized, results_fedavg, on=\"dataset\", suffixes=('_centralized', '_federated'))\n",
    "# merged_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for cross-silo experiment with Value experiment\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=18,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_silo_value_race_{model}.pdf\")\n",
    "\n",
    "# Before and after for cross-silo experiment with Value experiment\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_silo_value_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=results_df, title=\"FedAvg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=dp.loc[dp[\"model\"] == \"LogisticRegression\"], title=\"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = dp[dp[\"model\"] == \"LogisticRegression\"]\n",
    "dp_sex_unfairness = [(row[\"dataset\"], row[\"value_DP_SEX\"].split(\"_\")[-1], float(row[\"DP_SEX\"])) for _, row in logistic_regression.iterrows()]\n",
    "\n",
    "df_sex_unfairness_logistic = pd.DataFrame(dp_sex_unfairness, columns =['dataset', 'Value', 'DP_SEX'])\n",
    "\n",
    "logistic_regression = dp[dp[\"model\"] == \"LogisticRegression\"]\n",
    "dp_race_unfairness = [(row[\"dataset\"], row[\"value_DP_RACE\"].split(\"_\")[-1], float(row[\"DP_RACE\"])) for _, row in logistic_regression.iterrows()]\n",
    "\n",
    "df_race_unfairness_logistic = pd.DataFrame(dp_race_unfairness, columns =['dataset', 'Value', 'DP_RACE'])\n",
    "\n",
    "\n",
    "xgboost = dp[dp[\"model\"] == \"XGBoost\"]\n",
    "dp_sex_unfairness = [(row[\"dataset\"], row[\"value_DP_SEX\"].split(\"_\")[-1], float(row[\"DP_SEX\"])) for _, row in xgboost.iterrows()]\n",
    "\n",
    "df_sex_unfairness_xgboost = pd.DataFrame(dp_sex_unfairness, columns =['dataset', 'Value', 'DP_SEX'])\n",
    "\n",
    "xgboost = dp[dp[\"model\"] == \"XGBoost\"]\n",
    "dp_race_unfairness = [(row[\"dataset\"], row[\"value_DP_RACE\"].split(\"_\")[-1], float(row[\"DP_RACE\"])) for _, row in xgboost.iterrows()]\n",
    "\n",
    "df_race_unfairness_xgboost = pd.DataFrame(dp_race_unfairness, columns =['dataset', 'Value', 'DP_RACE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba49d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "create_value_plot(df_sex_unfairness_logistic, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_SEX\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_sex_logistic_dem_parity_value\", save=True)\n",
    "create_value_plot(df_race_unfairness_logistic, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_RACE\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_race_logistic_dem_parity_value\", save=True)\n",
    "\n",
    "create_value_plot(df_sex_unfairness_xgboost, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_SEX\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_sex_xgboost_dem_parity_value\", save=True)\n",
    "create_value_plot(df_race_unfairness_xgboost, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_RACE\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_race_xgboost_dem_parity_value\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"bar_plot_differences_logistic_cross_silo_value\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"bar_plot_differences_xgboost_cross_silo_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62358999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race_unfairness_xgboost[\"Value\"] = [int(float(i)) for i in df_race_unfairness_xgboost[\"Value\"]]\n",
    "fig = visualize_value_change(df1=df_race_unfairness_xgboost, df2=results_df, title=\"Change in Max. Value Disparity\", y_label=\"Dem.Disparity\", font_size=26, ticks_font_size=20, initial_state=\"XGBoost\", jitter_amount=0.1)\n",
    "\n",
    "fig.savefig(f\"./images/arrow_silo_value_xgboost.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race_unfairness_logistic[\"Value\"] = [int(float(i)) for i in df_race_unfairness_logistic[\"Value\"]]\n",
    "fig = visualize_value_change(df1=df_race_unfairness_logistic, df2=results_df, title=\"Change in Max. Value Disparity\", y_label=\"Dem.Disparity\", font_size=26, ticks_font_size=20, initial_state=\"Log. Regression\")\n",
    "\n",
    "fig.savefig(f\"./images/arrow_silo_value_lr.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3218980",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_1 = {row[\"dataset\"]:row[\"Value\"] for index, row in df_race_unfairness_logistic.iterrows()}\n",
    "values_2 = {row[\"dataset\"]:row[\"Value\"] for index, row in results_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_1 = {row[\"dataset\"]:row[\"Value\"] for index, row in df_race_unfairness_logistic.iterrows()}\n",
    "values_2 = {row[\"dataset\"]:row[\"Value\"] for index, row in results_df.iterrows()}\n",
    "counter = 0\n",
    "for key in values_1:\n",
    "    if values_1[key] != values_2[key]:\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cf05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b20bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da8d64fd",
   "metadata": {},
   "source": [
    "## Attribute Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f25029",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_attribute_cross_silo\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_silo_attribute_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/reb9abt2\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "states_unfairness = {\n",
    "    \"sex_states\":['SD', 'IN', 'WV', 'PA', 'IL', 'MI', 'WA', 'TX', 'MO', 'WY', 'TN', 'OK', 'UT', 'ID', 'ND', 'VA', 'AR', 'KS', 'NH', 'OH', 'LA'],\n",
    "    \"race_state\":['AL', 'NM', 'IA', 'MA', 'FL', 'AZ', 'NY', 'AK', 'MS', 'NC', 'GA', 'VT', 'SC', 'NJ', 'CT', 'DE', 'RI', 'WI', 'OR', 'NV', 'NE', 'MN', 'CA', 'MT', 'MD', 'CO', 'HI', 'KY', 'ME']\n",
    "}\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a650b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 51):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)].split(\"_\")[0]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_sex_centralized = dp[dp[\"model\"] == \"LogisticRegression\"][[\"DP_RACE\", \"dataset\"]]\n",
    "# results_fedavg = results_df[[\"DP_RACE\", \"dataset\"]]\n",
    "\n",
    "# # Merge the two DataFrames on the \"dataset\" column\n",
    "# merged_df = pd.merge(dp_sex_centralized, results_fedavg, on=\"dataset\", suffixes=('_centralized', '_federated'))\n",
    "# merged_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for the attribute experiment with Cross-silo\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Parity\",\n",
    "        xlabel=\"FedAVG Dem. Parity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_silo_attribute_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Parity\",\n",
    "        xlabel=\"FedAVG Dem. Parity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_silo_attribute_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b80301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP Sex on the y and DP Race on the X\n",
    "for model in dp[\"model\"].unique():\n",
    "    fig = scatter_fairness_plot(\n",
    "        df1=dp.loc[dp[\"model\"] == model],\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column_X=\"DP_RACE\",\n",
    "        fairness_column_Y=\"DP_SEX\",\n",
    "        ylabel=f\"Dem. Disparity SEX\",\n",
    "        xlabel=\"Dem. Disparity RACE\",\n",
    "        title=f\"Attribute Bias Distribution\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "\n",
    "    fig.savefig(f\"./images/{model}_attribute.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ffc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\")\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49829100",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"bar_plot_differences_logistic_cross_silo_attribute\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"bar_plot_differences_xgboost_cross_silo_attribute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673d6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ae0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc9f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e52311d9",
   "metadata": {},
   "source": [
    "# Cross-Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde094c",
   "metadata": {},
   "source": [
    "## Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_attribute_cross_device\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_device_attribute_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/7xd7s7vf\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "states_unfairness = {\n",
    "    \"sex_states\":['IN_0', 'IN_2', 'IN_3', 'WV_0', 'PA_0', 'PA_1', 'PA_2', 'PA_3', 'PA_5', 'IL_2', 'IL_3', 'IL_4', 'MI_0', 'MI_1', 'MI_2', 'MI_3', 'MI_4', 'WA_3', 'WA_5', 'TX_0', 'TX_1', 'TX_3', 'TX_4', 'TX_5', 'MO_1', 'MO_4', 'TN_2', 'TN_4', 'TN_5', 'OK_0', 'OK_2', 'OK_3', 'NE_5', 'UT_0', 'UT_4', 'UT_5', 'ID_0', 'ND_1', 'ND_4', 'VA_0', 'VA_1', 'VA_3', 'VA_4', 'KS_2', 'KS_4', 'NH_0', 'NH_1', 'NH_4', 'NH_5', 'OH_0', 'OH_1', 'OH_2', 'OH_3', 'OH_4', 'LA_4'],\n",
    "    \"race_state\":['AL_2', 'AL_3', 'AL_4', 'NM_1', 'NM_5', 'SD_3', 'IA_0', 'IA_1', 'MA_0', 'MA_3', 'MA_4', 'WV_5', 'FL_5', 'AZ_0', 'AZ_5', 'NY_0', 'NY_1', 'NY_2', 'NY_3', 'NY_4', 'NY_5', 'MS_4', 'MS_5', 'NC_1', 'SC_0', 'SC_1', 'SC_2', 'SC_4', 'NJ_1', 'NJ_3', 'NJ_4', 'CT_2', 'CT_5', 'RI_2', 'WI_2', 'WI_3', 'OR_1', 'OR_3', 'OR_4', 'NV_0', 'NV_1', 'NV_3', 'NE_3', 'NE_4', 'UT_1', 'MN_0', 'MN_3', 'MN_4', 'MN_5', 'ID_2', 'CA_1', 'CO_0', 'CO_3', 'LA_2', 'HI_0', 'ME_1'],\n",
    "}\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 111):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_sex_centralized = dp[dp[\"model\"] == \"LogisticRegression\"][[\"DP_RACE\", \"dataset\"]]\n",
    "# results_fedavg = results_df[[\"DP_RACE\", \"dataset\"]]\n",
    "\n",
    "# # Merge the two DataFrames on the \"dataset\" column\n",
    "# merged_df = pd.merge(dp_sex_centralized, results_fedavg, on=\"dataset\", suffixes=('_centralized', '_federated'))\n",
    "# merged_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for the attribute experiment with Cross-silo\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model],\n",
    "        df2=results_df,\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_device_attribute_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_device_attribute_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP Sex on the y and DP Race on the X\n",
    "for model in dp[\"model\"].unique():\n",
    "    fig = scatter_fairness_plot(\n",
    "        df1=dp.loc[dp[\"model\"] == model],\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column_X=\"DP_RACE\",\n",
    "        fairness_column_Y=\"DP_SEX\",\n",
    "        ylabel=f\"Dem. Disparity SEX\",\n",
    "        xlabel=\"Dem. Disparity RACE\",\n",
    "        title=f\"Attribute Bias Distribution\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "\n",
    "    fig.savefig(f\"./images/{model}_attribute_cross_device.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"bar_plot_differences_logistic_cross_device_attribute\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"bar_plot_differences_xgboost_cross_device_attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395be719",
   "metadata": {},
   "source": [
    "## Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db86948",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_value_cross_device\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_device_value_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/dyefdq2e\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 100):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa72775",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3047689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_sex_centralized = dp[dp[\"model\"] == \"LogisticRegression\"][[\"DP_RACE\", \"dataset\"]]\n",
    "# results_fedavg = results_df[[\"DP_RACE\", \"dataset\"]]\n",
    "\n",
    "# # Merge the two DataFrames on the \"dataset\" column\n",
    "# merged_df = pd.merge(dp_sex_centralized, results_fedavg, on=\"dataset\", suffixes=('_centralized', '_federated'))\n",
    "# merged_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for cross-silo experiment with Value experiment\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_device_value_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "    )\n",
    "    plot.savefig(f\"./images/before_after_cross_device_value_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=results_df, title=\"FedAvg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=dp.loc[dp[\"model\"] == \"LogisticRegression\"], title=\"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = dp[dp[\"model\"] == \"LogisticRegression\"]\n",
    "dp_sex_unfairness = [(row[\"dataset\"], row[\"value_DP_SEX\"].split(\"_\")[-1], float(row[\"DP_SEX\"])) for _, row in logistic_regression.iterrows()]\n",
    "\n",
    "df_sex_unfairness_logistic = pd.DataFrame(dp_sex_unfairness, columns =['dataset', 'Value', 'DP_SEX'])\n",
    "\n",
    "logistic_regression = dp[dp[\"model\"] == \"LogisticRegression\"]\n",
    "dp_race_unfairness = [(row[\"dataset\"], row[\"value_DP_RACE\"].split(\"_\")[-1], float(row[\"DP_RACE\"])) for _, row in logistic_regression.iterrows()]\n",
    "\n",
    "df_race_unfairness_logistic = pd.DataFrame(dp_race_unfairness, columns =['dataset', 'Value', 'DP_RACE'])\n",
    "\n",
    "\n",
    "xgboost = dp[dp[\"model\"] == \"XGBoost\"]\n",
    "dp_sex_unfairness = [(row[\"dataset\"], row[\"value_DP_SEX\"].split(\"_\")[-1], float(row[\"DP_SEX\"])) for _, row in xgboost.iterrows()]\n",
    "\n",
    "df_sex_unfairness_xgboost = pd.DataFrame(dp_sex_unfairness, columns =['dataset', 'Value', 'DP_SEX'])\n",
    "\n",
    "xgboost = dp[dp[\"model\"] == \"XGBoost\"]\n",
    "dp_race_unfairness = [(row[\"dataset\"], row[\"value_DP_RACE\"].split(\"_\")[-1], float(row[\"DP_RACE\"])) for _, row in xgboost.iterrows()]\n",
    "\n",
    "df_race_unfairness_xgboost = pd.DataFrame(dp_race_unfairness, columns =['dataset', 'Value', 'DP_RACE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "create_value_plot(df_sex_unfairness_logistic, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_SEX\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_sex_logistic_dem_parity_value_cross_device\", save=True)\n",
    "create_value_plot(df_race_unfairness_logistic, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_RACE\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_race_logistic_dem_parity_value_cross_device\", save=True)\n",
    "\n",
    "create_value_plot(df_sex_unfairness_xgboost, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_SEX\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_sex_xgboost_dem_parity_value_cross_device\", save=True)\n",
    "create_value_plot(df_race_unfairness_xgboost, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_RACE\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"dp_race_xgboost_dem_parity_value_cross_device\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe968047",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Demographic Parity\", save=True, fig_name=\"bar_plot_differences_logistic_cross_device_value\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Demographic Parity\", save=True, fig_name=\"bar_plot_differences_xgboost_cross_device_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = results_df[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e66204",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df_race_unfairness_xgboost[df_race_unfairness_xgboost[\"dataset\"].isin(states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19876320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race_unfairness_xgboost[\"Value\"] = [int(float(i)) for i in df_race_unfairness_xgboost[\"Value\"]]\n",
    "fig = visualize_value_change(df1=filtered, df2=results_df, title=\"Change in Max. Value Disparity\", y_label=\"Dem.Disparity\", font_size=26, ticks_font_size=20, initial_state=\"XGBoost\")\n",
    "\n",
    "fig.savefig(f\"./images/arrow_device_value_xgboost.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078af43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df_race_unfairness_logistic[df_race_unfairness_logistic[\"dataset\"].isin(states)]\n",
    "print(filtered)\n",
    "values_1 = {row[\"dataset\"]:row[\"Value\"] for index, row in filtered.iterrows()}\n",
    "values_2 = {row[\"dataset\"]:row[\"Value\"] for index, row in results_df.iterrows()}\n",
    "counter = 0\n",
    "for key in values_1:\n",
    "    if key in values_2 and values_1[key] != values_2[key]:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "df_race_unfairness_logistic[\"Value\"] = [int(float(i)) for i in df_race_unfairness_logistic[\"Value\"]]\n",
    "fig = visualize_value_change(df1=filtered, df2=results_df, title=\"Change in Max. Value Disparity\", y_label=\"Dem.Disparity\", font_size=26, ticks_font_size=20, initial_state=\"Log. Regression\")\n",
    "\n",
    "fig.savefig(f\"./images/arrow_device_value_lr.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633de1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181a812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd98d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409d938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2291a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca80c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bee20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd711a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408872ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d785c46d",
   "metadata": {},
   "source": [
    "# Unfairness Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630c917",
   "metadata": {},
   "source": [
    "## Cross-Silo Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_attribute_cross_silo\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_silo_attribute_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/zw4xdavp\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911796d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_unfairness = {\n",
    "    \"sex_states\": ['SD', 'IN', 'WV', 'PA', 'IL', 'MI', 'WA', 'TX', 'MO', 'WY', 'TN', 'OK', 'UT', 'ID', 'ND', 'VA', 'AR', 'KS', 'NH', 'OH', 'LA'],\n",
    "    \"race_state\": ['AL', 'NM', 'IA', 'MA', 'FL', 'AZ', 'NY', 'AK', 'MS', 'NC', 'GA', 'VT', 'SC', 'NJ', 'CT', 'DE', 'RI', 'WI', 'OR', 'NV', 'NE', 'MN', 'CA', 'MT', 'MD', 'CO', 'HI', 'KY', 'ME']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 51):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)].split(\"_\")[0]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67352cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=results_df, title=\"Unfairness reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7502cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for the attribute experiment with Cross-silo\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model],\n",
    "        df2=results_df,\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_cross_silo_attribute_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_cross_silo_attribute_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_logistic_cross_silo_attribute_puffle\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_xgboost_cross_silo_attribute_puffle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062d8ac",
   "metadata": {},
   "source": [
    "## Cross-Silo Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_value_cross_silo\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_silo_value_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/g0bvrdjf\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313488a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 51):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)].split(\"_\")[0]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a98f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=results_df, title=\"Unfairness reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c246788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for the attribute experiment with Cross-silo\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model],\n",
    "        df2=results_df,\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_silo_value_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_silo_value_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_logistic_cross_silo_value_puffle\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_xgboost_cross_silo_value_puffle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98110414",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = dp[dp[\"model\"] == \"LogisticRegression\"]\n",
    "dp_sex_unfairness = [(row[\"dataset\"], row[\"value_DP_SEX\"].split(\"_\")[-1], float(row[\"DP_SEX\"])) for _, row in logistic_regression.iterrows()]\n",
    "\n",
    "df_sex_unfairness_logistic = pd.DataFrame(dp_sex_unfairness, columns =['dataset', 'Value', 'DP_SEX'])\n",
    "\n",
    "logistic_regression = dp[dp[\"model\"] == \"LogisticRegression\"]\n",
    "dp_race_unfairness = [(row[\"dataset\"], row[\"value_DP_RACE\"].split(\"_\")[-1], float(row[\"DP_RACE\"])) for _, row in logistic_regression.iterrows()]\n",
    "\n",
    "df_race_unfairness_logistic = pd.DataFrame(dp_race_unfairness, columns =['dataset', 'Value', 'DP_RACE'])\n",
    "\n",
    "\n",
    "xgboost = dp[dp[\"model\"] == \"XGBoost\"]\n",
    "dp_sex_unfairness = [(row[\"dataset\"], row[\"value_DP_SEX\"].split(\"_\")[-1], float(row[\"DP_SEX\"])) for _, row in xgboost.iterrows()]\n",
    "\n",
    "df_sex_unfairness_xgboost = pd.DataFrame(dp_sex_unfairness, columns =['dataset', 'Value', 'DP_SEX'])\n",
    "\n",
    "xgboost = dp[dp[\"model\"] == \"XGBoost\"]\n",
    "dp_race_unfairness = [(row[\"dataset\"], row[\"value_DP_RACE\"].split(\"_\")[-1], float(row[\"DP_RACE\"])) for _, row in xgboost.iterrows()]\n",
    "\n",
    "df_race_unfairness_xgboost = pd.DataFrame(dp_race_unfairness, columns =['dataset', 'Value', 'DP_RACE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_value_plot(df_sex_unfairness_logistic, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_SEX\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"puffle_dp_sex_logistic_dem_parity_value_cross_silo\", save=True)\n",
    "create_value_plot(df_race_unfairness_logistic, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_RACE\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"puffle_dp_race_logistic_dem_parity_value_cross_silo\", save=True)\n",
    "\n",
    "create_value_plot(df_sex_unfairness_xgboost, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_SEX\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"puffle_dp_sex_xgboost_dem_parity_value_cross_silo\", save=True)\n",
    "create_value_plot(df_race_unfairness_xgboost, y_label=\"Dem. Disparity\", title=\"Value Bias Distribution\", attribute=\"DP_RACE\", font_size_labels=26, font_size_title=26, font_size_ticks=20, file_name=\"puffle_dp_race_xgboost_dem_parity_value_cross_silo\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Demographic Parity\", save=True, fig_name=\"puffle_bar_plot_differences_logistic_cross_silo_value\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Demographic Parity\", save=True, fig_name=\"puffle_bar_plot_differences_xgboost_cross_silo_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = results_df[\"dataset\"]\n",
    "\n",
    "filtered = df_race_unfairness_logistic[df_race_unfairness_logistic[\"dataset\"].isin(states)]\n",
    "print(filtered)\n",
    "values_1 = {row[\"dataset\"]:row[\"Value\"] for index, row in filtered.iterrows()}\n",
    "values_2 = {row[\"dataset\"]:row[\"Value\"] for index, row in results_df.iterrows()}\n",
    "counter = 0\n",
    "for key in values_1:\n",
    "    if key in values_2 and values_1[key] != values_2[key]:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "df_race_unfairness_logistic[\"Value\"] = [int(float(i)) for i in df_race_unfairness_logistic[\"Value\"]]\n",
    "fig = visualize_value_change(df1=filtered, df2=results_df, title=\"Change in Max. Value Disparity\", y_label=\"Dem.Disparity\", font_size=26, ticks_font_size=20, initial_state=\"Log. Regression\")\n",
    "\n",
    "fig.savefig(f\"./images/puffle_arrow_silo_value_lr.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = results_df[\"dataset\"]\n",
    "\n",
    "filtered = df_race_unfairness_xgboost[df_race_unfairness_xgboost[\"dataset\"].isin(states)]\n",
    "print(filtered)\n",
    "values_1 = {row[\"dataset\"]:row[\"Value\"] for index, row in filtered.iterrows()}\n",
    "values_2 = {row[\"dataset\"]:row[\"Value\"] for index, row in results_df.iterrows()}\n",
    "counter = 0\n",
    "for key in values_1:\n",
    "    if key in values_2 and values_1[key] != values_2[key]:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "df_race_unfairness_xgboost[\"Value\"] = [int(float(i)) for i in df_race_unfairness_xgboost[\"Value\"]]\n",
    "fig = visualize_value_change(df1=filtered, df2=results_df, title=\"Change in Max. Value Disparity\", y_label=\"Dem.Disparity\", font_size=26, ticks_font_size=20, initial_state=\"XGBoost\")\n",
    "\n",
    "fig.savefig(f\"./images/puffle_arrow_silo_value_xgboost.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f20939",
   "metadata": {},
   "source": [
    "## Cross-Device Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_attribute_cross_device\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_device_attribute_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/qu2rhc1e\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}\n",
    "states_unfairness = {\n",
    "    \"sex_states\": ['IN_0', 'IN_2', 'IN_3', 'WV_0', 'PA_0', 'PA_1', 'PA_2', 'PA_3', 'PA_5', 'IL_2', 'IL_3', 'IL_4', 'MI_0', 'MI_1', 'MI_2', 'MI_3', 'MI_4', 'WA_3', 'WA_5', 'TX_0', 'TX_1', 'TX_3', 'TX_4', 'TX_5', 'MO_1', 'MO_4', 'TN_2', 'TN_4', 'TN_5', 'OK_0', 'OK_2', 'OK_3', 'NE_5', 'UT_0', 'UT_4', 'UT_5', 'ID_0', 'ND_1', 'ND_4', 'VA_0', 'VA_1', 'VA_3', 'VA_4', 'KS_2', 'KS_4', 'NH_0', 'NH_1', 'NH_4', 'NH_5', 'OH_0', 'OH_1', 'OH_2', 'OH_3', 'OH_4', 'LA_4'],\n",
    "    \"race_state\": ['AL_2', 'AL_3', 'AL_4', 'NM_1', 'NM_5', 'SD_3', 'IA_0', 'IA_1', 'MA_0', 'MA_3', 'MA_4', 'WV_5', 'FL_5', 'AZ_0', 'AZ_5', 'NY_0', 'NY_1', 'NY_2', 'NY_3', 'NY_4', 'NY_5', 'MS_4', 'MS_5', 'NC_1', 'SC_0', 'SC_1', 'SC_2', 'SC_4', 'NJ_1', 'NJ_3', 'NJ_4', 'CT_2', 'CT_5', 'RI_2', 'WI_2', 'WI_3', 'OR_1', 'OR_3', 'OR_4', 'NV_0', 'NV_1', 'NV_3', 'NE_3', 'NE_4', 'UT_1', 'MN_0', 'MN_3', 'MN_4', 'MN_5', 'ID_2', 'CA_1', 'CO_0', 'CO_3', 'LA_2', 'HI_0', 'ME_1']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34074b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 111):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for the attribute experiment with Cross-silo\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model],\n",
    "        df2=results_df,\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_cross_device_attribute_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "        unfairness_distribution=states_unfairness,\n",
    "\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_cross_device_attribute_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_logistic_cross_device_attribute_puffle\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_xgboost_cross_device_attribute_puffle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a0753",
   "metadata": {},
   "source": [
    "## Cross-Device Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"baseline_value_cross_device\"\n",
    "dp = pd.read_csv(f'./{folder_name}/model_perf_DP.csv')\n",
    "eo = pd.read_csv(f'./{folder_name}/model_perf_EO.csv')\n",
    "with open ('../data/cross_device_value_final/FL_data/federated/partitions_names.json') as f:\n",
    "    partition_names = json.load(f)\n",
    "api = wandb.Api()\n",
    "run = api.run(\"/lucacorbucci/Fair_Fed_Dataset_results/runs/0rmxjt18\")\n",
    "df = pd.DataFrame(run.scan_history())\n",
    "\n",
    "attributes = [\"- First DP NEW.\", \"- Second DP NEW.\", \"- Third DP NEW.\", \"- Acc.\", \"- Group 3\"]\n",
    "mapping = {\n",
    "    \"- Third DP NEW.\": \"DP_RACE\",\n",
    "    \"- Second DP NEW.\": \"DP_MAR\",\n",
    "    \"- First DP NEW.\": \"DP_SEX\",\n",
    "    \"- Acc.\": \"accuracy\",\n",
    "    \"- Group 3\": \"Value\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0128f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for node in range(0, 200):\n",
    "    for attribute in attributes:\n",
    "        current_attribute = f\"{attribute_name} {node} {attribute}\"\n",
    "        if current_attribute in df.columns:\n",
    "            # Get the values for the current attribute\n",
    "            values = df[current_attribute].values\n",
    "            # remove the NaN values\n",
    "            values = values[~pd.isna(values)]\n",
    "            # get the last value\n",
    "            last_value = values[-1]\n",
    "            if node not in results:\n",
    "                results[node] = {}\n",
    "            results[node][mapping[attribute]] = last_value\n",
    "            results[node][\"dataset\"] = partition_names[str(node)]\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_comparison_fairness(df=results_df, title=\"Unfairness reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d65d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after for the attribute experiment with Cross-silo\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model],\n",
    "        df2=results_df,\n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_RACE\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"RACE\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_cross_device_attribute_race_{model}.pdf\")\n",
    "\n",
    "for model in dp[\"model\"].unique():\n",
    "    model_title = \"Log. Regr.\" if model == \"LogisticRegression\" else model\n",
    "\n",
    "    plot = local_client_fairness_plot(\n",
    "        df1=dp[dp[\"model\"] == model], \n",
    "        df2=results_df, \n",
    "        client_column=\"dataset\",\n",
    "        fairness_column=\"DP_SEX\",\n",
    "        ylabel=f\"{model_title} Dem. Disparity\",\n",
    "        xlabel=\"FedAVG Dem. Disparity\",\n",
    "        title=\"SEX\",\n",
    "        title_font_size=26, \n",
    "        label_font_size=26,\n",
    "        ticks_font_size=20,\n",
    "\n",
    "    )\n",
    "    plot.savefig(f\"./images/puffle_before_after_cross_device_attribute_sex_{model}.pdf\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff677fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"LogisticRegression\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"Logistic Regression - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_logistic_cross_silo_attribute_puffle\")\n",
    "\n",
    "diff_df = compute_differences(dp.loc[dp[\"model\"] == \"XGBoost\"], results_df)\n",
    "diff_df = diff_df.dropna(subset=[\"DP_SEX\"])\n",
    "bar_plot_differences(diff_df, list(diff_df[\"dataset\"]), title=\"XGBoost - FedAvg (Puffle)\", font_size_title=26, font_size_ticks=20, font_size_labels=26, y_axis=\"Dem. Disparity\", save=True, fig_name=\"puffle_bar_plot_differences_xgboost_cross_silo_attribute_puffle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97eb0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f21f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0fb41c3",
   "metadata": {},
   "source": [
    "# Dataset Disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b41a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_process_income(df):\n",
    "#     \"\"\"\n",
    "#     Pre-process the income dataset to make it ready for the simulation\n",
    "#     In this function we consider \"SEX\" as the sensitive value and \"PINCP\" as the target value.\n",
    "\n",
    "#     Args:\n",
    "#         data: the raw data\n",
    "#         years_list: the list of years to be considered\n",
    "#         states_list: the list of states to be considered\n",
    "\n",
    "#     Returns:\n",
    "#         Returns a list of pre-processed data for each state, if multiple years are\n",
    "#         selected, the data are concatenated.\n",
    "#         We return three lists:\n",
    "#         - The first list contains a pandas dataframe of features for each state\n",
    "#         - The second list contains a pandas dataframe of labels for each state\n",
    "#         - The third list contains a pandas dataframe of groups for each state\n",
    "#         The values in the list are numpy array of the dataframes\n",
    "#     \"\"\"\n",
    "\n",
    "#     categorical_columns = [\"COW\", \"SCHL\"] #, \"RAC1P\"]\n",
    "#     continuous_columns = [\"AGEP\", \"WKHP\", \"OCCP\", \"POBP\", \"RELP\"]\n",
    "\n",
    "#     # get the target and sensitive attributes\n",
    "#     target_attributes = df[\">50K\"]\n",
    "#     sensitive_attributes = df[\"SEX\"]\n",
    "\n",
    "#     # convert the columns to one-hot encoding\n",
    "#     df = pd.get_dummies(df, columns=categorical_columns, dtype=int)\n",
    "\n",
    "#     # normalize the continuous columns between 0 and 1\n",
    "#     for col in continuous_columns:\n",
    "#         df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "#     return pd.DataFrame(df)\n",
    "\n",
    "\n",
    "# def pre_process_single_datasets(df):\n",
    "#     dataframe = pd.DataFrame()\n",
    "#     label = pd.DataFrame()\n",
    "#     group = pd.DataFrame()\n",
    "#     second_group = pd.DataFrame()\n",
    "#     third_group = pd.DataFrame()\n",
    "#     dataframes = []\n",
    "#     labels = []\n",
    "#     groups = []\n",
    "#     second_groups = []\n",
    "#     third_groups = []\n",
    "#     target_attributes = df[\">50K\"]\n",
    "#     sensitive_attributes = df[\"SEX\"]\n",
    "#     second_sensitive_attributes = df[\"MAR\"]\n",
    "    \n",
    "#     third_sensitive_attributes = df[\"RAC1P\"]\n",
    "#     third_sensitive_attributes = third_sensitive_attributes.astype(int)\n",
    "#     target_attributes = target_attributes.astype(int)\n",
    "\n",
    "#     sensitive_attributes = [1 if item == 1 else 0 for item in sensitive_attributes]\n",
    "\n",
    "#     second_sensitive_attributes = [\n",
    "#         1 if item == 1 else 0 for item in second_sensitive_attributes\n",
    "#     ]\n",
    "\n",
    "#     third_sensitive_attributes = [\n",
    "#         1 if item == 1 else 0 for item in third_sensitive_attributes\n",
    "#     ]\n",
    "\n",
    "#     df = df.drop([\">50K\"], axis=1)\n",
    "#     # df.drop(['RAC1P_1.0', 'RAC1P_2.0'], axis=1, inplace=True)\n",
    "\n",
    "#     # concatenate the dataframes\n",
    "#     dataframe = pd.concat([dataframe, df])\n",
    "#     # remove RAC1P from dataframe\n",
    "\n",
    "#     # convert the labels and groups to dataframes\n",
    "#     label = pd.concat([label, pd.DataFrame(target_attributes)])\n",
    "#     group = pd.concat([group, pd.DataFrame(sensitive_attributes)])\n",
    "#     second_group = pd.concat([second_group, pd.DataFrame(second_sensitive_attributes)])\n",
    "#     third_group = pd.concat([third_group, pd.DataFrame(third_sensitive_attributes)])\n",
    "\n",
    "#     assert len(dataframe) == len(label) == len(group) == len(second_group)\n",
    "#     dataframes.append(dataframe.to_numpy())\n",
    "#     labels.append(label.to_numpy())\n",
    "#     groups.append(group.to_numpy())\n",
    "#     second_groups.append(second_group.to_numpy())\n",
    "#     third_groups.append(third_group.to_numpy())\n",
    "#     return dataframes, labels, groups, second_groups, third_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # open the original csv files: \n",
    "# folder = \"../original_data/cross_silo_value_final/\"\n",
    "# list_files = !ls {folder}\n",
    "# unfair_dfs = []\n",
    "# print(list_files)\n",
    "\n",
    "# states = ['CT',\n",
    "#  'RI',\n",
    "#  'VT',\n",
    "#  'TX',\n",
    "#  'GA',\n",
    "#  'PR',\n",
    "#  'OH',\n",
    "#  'NE',\n",
    "#  'HI',\n",
    "#  'MO',\n",
    "#  'PA',\n",
    "#  'DE',\n",
    "#  'WV',\n",
    "#  'MD',\n",
    "#  'AZ',\n",
    "#  'LA',\n",
    "#  'WA',\n",
    "#  'TN',\n",
    "#  'MA',\n",
    "#  'NJ',\n",
    "#  'ME',\n",
    "#  'SC',\n",
    "#  'MI',\n",
    "#  'OK',\n",
    "#  'IL',\n",
    "#  'FL',\n",
    "#  'UT',\n",
    "#  'AK',\n",
    "#  'WI',\n",
    "#  'NH',\n",
    "#  'VA',\n",
    "#  'SD',\n",
    "#  'MS',\n",
    "#  'ND',\n",
    "#  'NC',\n",
    "#  'AL',\n",
    "#  'IA',\n",
    "#  'ID',\n",
    "#  'WY',\n",
    "#  'NV',\n",
    "#  'NM',\n",
    "#  'NY',\n",
    "#  'CA',\n",
    "#  'AR',\n",
    "#  'MN',\n",
    "#  'OR',\n",
    "#  'MT',\n",
    "#  'KY',\n",
    "#  'KS',\n",
    "#  'IN',\n",
    "#  'CO']\n",
    "\n",
    "# partitions_names = []\n",
    "\n",
    "# for state in states:\n",
    "#     partitions = set()\n",
    "#     for file in list_files:\n",
    "#         if file.endswith(\".csv\"):\n",
    "#             partition = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "#             if partition not in partitions:\n",
    "#                 partitions_names.append(f\"{state}_{partition}\")\n",
    "#                 partitions.add(partition)\n",
    "#                 train = pd.read_csv(f\"{folder}{state}_{partition}.csv\")\n",
    "#                 # split the train csv into train and test\n",
    "#                 train, test = train_test_split(train, test_size=0.2)\n",
    "\n",
    "#                 unfair_dfs.append(train)\n",
    "#                 unfair_dfs.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_df = pd.concat(unfair_dfs, ignore_index=True)\n",
    "# concatenated_df[\"PINCP\"] = [1 if item == True else 0 for item in concatenated_df[\"PINCP\"]]\n",
    "\n",
    "# # rename the column PINCP to >50K\n",
    "# concatenated_df.rename(columns={\"PINCP\": \">50K\"}, inplace=True)\n",
    "# concatenated_df.drop([\"__index_level_0__\"], axis=1, inplace=True)\n",
    "\n",
    "# concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b725764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply one-hot encoding\n",
    "# pre_processed_df = pre_process_income(concatenated_df)\n",
    "\n",
    "# split_dfs = []\n",
    "# start_idx = 0\n",
    "# for df in unfair_dfs:\n",
    "#     end_idx = start_idx + len(df)\n",
    "#     split_dfs.append(pre_processed_df.iloc[start_idx:end_idx])\n",
    "#     start_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def demographic_parity_difference(y_pred, sensitive_features):\n",
    "#     \"\"\"\n",
    "#     Computes the demographic parity difference between two numpy arrays.\n",
    "\n",
    "#     This function calculates the demographic parity difference, which is the\n",
    "#     largest absolute difference in the acceptance rates between any two groups\n",
    "#     defined by the sensitive features.\n",
    "\n",
    "#     Args:\n",
    "#         y_pred : array-like of shape (n_samples,)\n",
    "#             Predicted labels.\n",
    "#         sensitive_features : array-like of shape (n_samples,)\n",
    "#             Sensitive features.\n",
    "\n",
    "#     Returns:\n",
    "#         float\n",
    "#             The demographic parity difference.\n",
    "#     \"\"\"\n",
    "#     # Ensure input arrays are numpy arrays\n",
    "#     y_pred = np.asarray(y_pred)\n",
    "#     sensitive_features = np.asarray(sensitive_features)\n",
    "\n",
    "#     # Check for consistent lengths\n",
    "#     if not (y_pred.shape == sensitive_features.shape):\n",
    "#         raise ValueError(\"y_pred and sensitive_features must have the same shape\")\n",
    "\n",
    "#     # Get unique values in the sensitive features array\n",
    "#     unique_sensitive_values = np.unique(sensitive_features)\n",
    "\n",
    "#     # Calculate acceptance rates for each group\n",
    "#     acceptance_rates = []\n",
    "#     for group_value in unique_sensitive_values:\n",
    "#         group_indices = (sensitive_features == group_value)\n",
    "#         acceptance_rate = np.mean(y_pred[group_indices])\n",
    "#         acceptance_rates.append(acceptance_rate)\n",
    "\n",
    "#     # Calculate the demographic parity difference\n",
    "#     max_diff = 0\n",
    "#     for i in range(len(acceptance_rates)):\n",
    "#         for j in range(i + 1, len(acceptance_rates)):\n",
    "#             diff = np.abs(acceptance_rates[i] - acceptance_rates[j])\n",
    "#             if diff > max_diff:\n",
    "#                 max_diff = diff\n",
    "\n",
    "#     return max_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_analysis = {}\n",
    "\n",
    "# counter = {\n",
    "#     \"sex\": 0,\n",
    "#     \"race\": 0, \n",
    "#     \"mar\": 0,\n",
    "# }\n",
    "\n",
    "# for index in range(0, len(split_dfs), 2):\n",
    "#     train_state = split_dfs[index]\n",
    "#     test_state = split_dfs[index + 1]\n",
    "#     print(len(train_state), len(test_state))\n",
    "#     (\n",
    "#         train_data,\n",
    "#         train_labels,\n",
    "#         train_groups,\n",
    "#         train_second_groups,\n",
    "#         train_third_groups,\n",
    "#     ) = pre_process_single_datasets(train_state)\n",
    "#     (\n",
    "#         test_data,\n",
    "#         test_labels,\n",
    "#         test_groups,\n",
    "#         test_second_groups,\n",
    "#         test_third_groups,\n",
    "#     ) = pre_process_single_datasets(test_state)\n",
    "\n",
    "\n",
    "#     SEX = test_groups[0]\n",
    "#     # MAR = test_second_groups[0]\n",
    "#     RACE = test_third_groups[0]\n",
    "#     labels = test_labels[0]\n",
    "\n",
    "#     dp_sex = demographic_parity_difference(SEX, labels)\n",
    "#     # dp_mar = demographic_parity_difference(MAR, labels)\n",
    "#     dp_race = demographic_parity_difference(RACE, labels)\n",
    "\n",
    "\n",
    "#     if dp_sex > dp_race:\n",
    "#         max_disparity = \"sex\" \n",
    "#     else: \n",
    "#         max_disparity = \"race\"\n",
    "    \n",
    "#     counter[max_disparity] += 1\n",
    "\n",
    "#     node = index // 2\n",
    "#     if node not in dataset_analysis:\n",
    "#         dataset_analysis[node] = {}\n",
    "#     dataset_analysis[node][mapping[attribute]] = last_value\n",
    "#     dataset_analysis[node][\"dataset\"] = partition_names[str(node)].split(\"_\")[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
