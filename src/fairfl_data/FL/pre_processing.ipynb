{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_income(df):\n",
    "    \"\"\"\n",
    "    Pre-process the income dataset to make it ready for the simulation\n",
    "    In this function we consider \"SEX\" as the sensitive value and \"PINCP\" as the target value.\n",
    "\n",
    "    Args:\n",
    "        data: the raw data\n",
    "        years_list: the list of years to be considered\n",
    "        states_list: the list of states to be considered\n",
    "\n",
    "    Returns:\n",
    "        Returns a list of pre-processed data for each state, if multiple years are\n",
    "        selected, the data are concatenated.\n",
    "        We return three lists:\n",
    "        - The first list contains a pandas dataframe of features for each state\n",
    "        - The second list contains a pandas dataframe of labels for each state\n",
    "        - The third list contains a pandas dataframe of groups for each state\n",
    "        The values in the list are numpy array of the dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    categorical_columns = [\"COW\", \"SCHL\"]  # , \"RAC1P\"]\n",
    "    continuous_columns = [\"AGEP\", \"WKHP\", \"OCCP\", \"POBP\", \"RELP\"]\n",
    "\n",
    "    # get the target and sensitive attributes\n",
    "    target_attributes = df[\">50K\"]\n",
    "    sensitive_attributes = df[\"SEX\"]\n",
    "\n",
    "    # convert the columns to one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "    # normalize the continuous columns between 0 and 1\n",
    "    for col in continuous_columns:\n",
    "        df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "\n",
    "def pre_process_single_datasets(df):\n",
    "    dataframe = pd.DataFrame()\n",
    "    label = pd.DataFrame()\n",
    "    group = pd.DataFrame()\n",
    "    second_group = pd.DataFrame()\n",
    "    dataframes = []\n",
    "    labels = []\n",
    "    groups = []\n",
    "    second_groups = []\n",
    "\n",
    "    target_attributes = df[\">50K\"]\n",
    "    sensitive_attributes = df[\"SEX\"]\n",
    "    second_sensitive_attributes = df[\"MAR\"]\n",
    "    target_attributes = target_attributes.astype(int)\n",
    "\n",
    "    sensitive_attributes = [1 if item == 1 else 0 for item in sensitive_attributes]\n",
    "\n",
    "    second_sensitive_attributes = [\n",
    "        1 if item == 1 else 0 for item in second_sensitive_attributes\n",
    "    ]\n",
    "\n",
    "    df = df.drop([\">50K\"], axis=1)\n",
    "\n",
    "    # concatenate the dataframes\n",
    "    dataframe = pd.concat([dataframe, df])\n",
    "    # convert the labels and groups to dataframes\n",
    "    label = pd.concat([label, pd.DataFrame(target_attributes)])\n",
    "    group = pd.concat([group, pd.DataFrame(sensitive_attributes)])\n",
    "    second_group = pd.concat([second_group, pd.DataFrame(second_sensitive_attributes)])\n",
    "\n",
    "    assert len(dataframe) == len(label) == len(group) == len(second_group)\n",
    "    dataframes.append(dataframe.to_numpy())\n",
    "    labels.append(label.to_numpy())\n",
    "    groups.append(group.to_numpy())\n",
    "    second_groups.append(second_group.to_numpy())\n",
    "    return dataframes, labels, groups, second_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018', 'AK_test_0.csv', 'AK_test_1.csv', 'AK_test_2.csv', 'AK_test_3.csv', 'AK_test_4.csv', 'AK_test_5.csv', 'AK_test_6.csv', 'AK_test_7.csv', 'AK_test_8.csv', 'AK_test_9.csv', 'AK_train_0.csv', 'AK_train_1.csv', 'AK_train_2.csv', 'AK_train_3.csv', 'AK_train_4.csv', 'AK_train_5.csv', 'AK_train_6.csv', 'AK_train_7.csv', 'AK_train_8.csv', 'AK_train_9.csv', 'AK_val_0.csv', 'AK_val_1.csv', 'AK_val_2.csv', 'AK_val_3.csv', 'AK_val_4.csv', 'AK_val_5.csv', 'AK_val_6.csv', 'AK_val_7.csv', 'AK_val_8.csv', 'AK_val_9.csv', 'CT_test_0.csv', 'CT_test_1.csv', 'CT_test_2.csv', 'CT_test_3.csv', 'CT_test_4.csv', 'CT_test_5.csv', 'CT_test_6.csv', 'CT_test_7.csv', 'CT_test_8.csv', 'CT_test_9.csv', 'CT_train_0.csv', 'CT_train_1.csv', 'CT_train_2.csv', 'CT_train_3.csv', 'CT_train_4.csv', 'CT_train_5.csv', 'CT_train_6.csv', 'CT_train_7.csv', 'CT_train_8.csv', 'CT_train_9.csv', 'CT_val_0.csv', 'CT_val_1.csv', 'CT_val_2.csv', 'CT_val_3.csv', 'CT_val_4.csv', 'CT_val_5.csv', 'CT_val_6.csv', 'CT_val_7.csv', 'CT_val_8.csv', 'CT_val_9.csv', 'FL_data']\n"
     ]
    }
   ],
   "source": [
    "folder = \"../data/\"\n",
    "list_files = !ls {folder}\n",
    "unfair_dfs = []\n",
    "print(list_files)\n",
    "\n",
    "states = [\"AK\", \"CT\"]\n",
    "\n",
    "partitions_names = []\n",
    "\n",
    "for state in states:\n",
    "    partitions = set()\n",
    "    for file in list_files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            partition = int(file.split(\"_\")[-1].split(\".\")[0])\n",
    "            if partition not in partitions:\n",
    "                partitions_names.append(f\"{state}_{partition}\")\n",
    "                partitions.add(partition)\n",
    "                train = pd.read_csv(f\"../data/{state}_train_{partition}.csv\")\n",
    "                val = pd.read_csv(f\"../data/{state}_val_{partition}.csv\")\n",
    "                concatenated_data = pd.concat([train, val])\n",
    "                unfair_dfs.append(concatenated_data)\n",
    "                df = pd.read_csv(f\"../data/{state}_test_{partition}.csv\")\n",
    "                unfair_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AK_0',\n",
       " 'AK_1',\n",
       " 'AK_2',\n",
       " 'AK_3',\n",
       " 'AK_4',\n",
       " 'AK_5',\n",
       " 'AK_6',\n",
       " 'AK_7',\n",
       " 'AK_8',\n",
       " 'AK_9',\n",
       " 'CT_0',\n",
       " 'CT_1',\n",
       " 'CT_2',\n",
       " 'CT_3',\n",
       " 'CT_4',\n",
       " 'CT_5',\n",
       " 'CT_6',\n",
       " 'CT_7',\n",
       " 'CT_8',\n",
       " 'CT_9']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitions_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(unfair_dfs, ignore_index=True)\n",
    "concatenated_df[\"PINCP\"] = [1 if item == True else 0 for item in concatenated_df[\"PINCP\"]]\n",
    "\n",
    "# rename the column PINCP to >50K\n",
    "concatenated_df.rename(columns={\"PINCP\": \">50K\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (303, 40) (54, 40)\n",
      "1 (302, 40) (53, 40)\n",
      "2 (301, 40) (53, 40)\n",
      "3 (301, 40) (53, 40)\n",
      "4 (301, 40) (53, 40)\n",
      "5 (301, 40) (53, 40)\n",
      "6 (301, 40) (53, 40)\n",
      "7 (301, 40) (53, 40)\n",
      "8 (301, 40) (53, 40)\n",
      "9 (301, 40) (53, 40)\n",
      "10 (1682, 40) (297, 40)\n",
      "11 (1682, 40) (297, 40)\n",
      "12 (1682, 40) (297, 40)\n",
      "13 (1682, 40) (297, 40)\n",
      "14 (1682, 40) (297, 40)\n",
      "15 (1682, 40) (297, 40)\n",
      "16 (1682, 40) (297, 40)\n",
      "17 (1681, 40) (296, 40)\n",
      "18 (1681, 40) (296, 40)\n",
      "19 (1680, 40) (296, 40)\n"
     ]
    }
   ],
   "source": [
    "# Apply one-hot encoding\n",
    "pre_processed_df = pre_process_income(concatenated_df)\n",
    "\n",
    "split_dfs = []\n",
    "start_idx = 0\n",
    "for df in unfair_dfs:\n",
    "    end_idx = start_idx + len(df)\n",
    "    split_dfs.append(pre_processed_df.iloc[start_idx:end_idx])\n",
    "    start_idx = end_idx\n",
    "\n",
    "for index in range(0, len(split_dfs), 2):\n",
    "    train_state = split_dfs[index]\n",
    "    test_state = split_dfs[index + 1]\n",
    "\n",
    "    (\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        train_groups,\n",
    "        train_second_groups,\n",
    "    ) = pre_process_single_datasets(train_state)\n",
    "    (\n",
    "        test_data,\n",
    "        test_labels,\n",
    "        test_groups,\n",
    "        test_second_groups,\n",
    "    ) = pre_process_single_datasets(test_state)\n",
    "\n",
    "    print(index // 2, train_data[0].shape, test_data[0].shape)\n",
    "\n",
    "    if not os.path.exists(\n",
    "        f\"../data/FL_data/federated/{index // 2}\"\n",
    "    ):\n",
    "        os.makedirs(f\"../data/FL_data/federated/{index // 2}\")\n",
    "        # save partitions_names \n",
    "    json_file = {index:data for index, data in enumerate(partitions_names)}\n",
    "    with open(f\"../data/FL_data/federated/partitions_names.json\", \"w\") as f:\n",
    "        json.dump(json_file, f)\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_dataframes_{index // 2}_train.npy\",\n",
    "        train_data[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_labels_{index // 2}_train.npy\",\n",
    "        train_labels[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_groups_{index // 2}_train.npy\",\n",
    "        train_groups[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_second_groups_{index // 2}_train.npy\",\n",
    "        train_second_groups[0],\n",
    "    )\n",
    "\n",
    "\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_dataframes_{index // 2}_test.npy\",\n",
    "        test_data[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_labels_{index // 2}_test.npy\",\n",
    "        test_labels[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_groups_{index // 2}_test.npy\",\n",
    "        test_groups[0],\n",
    "    )\n",
    "    np.save(\n",
    "        f\"../data/FL_data/federated/{index // 2}/income_second_groups_{index // 2}_test.npy\",\n",
    "        test_second_groups[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45569620253164556, 1.0, 0.23421588594704684, ..., True, False,\n",
       "        False],\n",
       "       [0.012658227848101266, 5.0, 0.40835030549898166, ..., False,\n",
       "        False, False],\n",
       "       [0.569620253164557, 1.0, 0.4318737270875764, ..., False, False,\n",
       "        False],\n",
       "       ...,\n",
       "       [0.27848101265822783, 1.0, 0.5081466395112016, ..., True, False,\n",
       "        False],\n",
       "       [0.569620253164557, 1.0, 0.3304480651731161, ..., False, False,\n",
       "        False],\n",
       "       [0.17721518987341772, 1.0, 0.36568228105906314, ..., False, False,\n",
       "        False]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6075949367088608, 1.0, 0.36578411405295314, ..., False, False,\n",
       "        False],\n",
       "       [0.21518987341772153, 5.0, 0.5244399185336049, ..., False, False,\n",
       "        False],\n",
       "       [0.4050632911392405, 1.0, 0.23421588594704684, ..., True, False,\n",
       "        False],\n",
       "       ...,\n",
       "       [0.3670886075949367, 1.0, 0.40020366598778007, ..., False, False,\n",
       "        False],\n",
       "       [0.24050632911392406, 1.0, 0.5613034623217923, ..., False, False,\n",
       "        False],\n",
       "       [0.5189873417721519, 5.0, 0.47963340122199594, ..., False, False,\n",
       "        False]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(\"../data/FL_data/federated/0/income_dataframes_0_train.npy\", allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
